# Provider Configuration Guide

AgentFlow provides a unified LLM Provider abstraction layer supporting 13+ major model providers with enterprise features like resilient failover, multi-provider routing, and API key pooling.

## Supported Providers

| Provider | Default Model | Default BaseURL | Features |
|----------|---------------|-----------------|----------|
| OpenAI | gpt-5.2 | https://api.openai.com | Tool calling, multimodal, Responses API |
| Anthropic | claude-opus-4.5 | https://api.anthropic.com | Long context, chain-of-thought, Thought Signatures |
| Gemini | gemini-3-pro | https://generativelanguage.googleapis.com | Multimodal, 1M tokens context |
| DeepSeek | deepseek-chat | https://api.deepseek.com | Cost-effective, deepseek-reasoner mode |
| Qwen | qwen3-235b-a22b | https://dashscope.aliyuncs.com | Chinese optimized, DashScope API |
| GLM | glm-4-plus | https://open.bigmodel.cn | Zhipu AI, Chinese optimized |
| Grok | grok-beta | https://api.x.ai | xAI, real-time info |
| MiniMax | abab6.5s-chat | https://api.minimax.io | XML tool call format |
| Mistral | mistral-large | https://api.mistral.ai | EU compliance, OpenAI compatible |
| Hunyuan | hunyuan-pro | https://api.hunyuan.cloud.tencent.com/v1 | Tencent, OpenAI compatible |
| Kimi | moonshot-v1 | https://api.moonshot.cn | Long context, OpenAI compatible |
| Llama | llama-3.3-70b | https://api.together.xyz | Multi-platform hosting (Together/Replicate/OpenRouter) |
| Doubao | Doubao-1.5-pro-32k | https://ark.cn-beijing.volces.com | ByteDance Volcano Engine |

## API Format Classification

### OpenAI Compatible API
The following providers use OpenAI compatible API, sharing the same request/response format:
- OpenAI, DeepSeek, Qwen, GLM, Grok, Mistral, Hunyuan, Kimi, Llama, Doubao

### Custom API
- **Anthropic Claude**: Uses `x-api-key` authentication, system message passed separately, different SSE streaming format
- **Google Gemini**: Uses `x-goog-api-key` authentication, message format uses `contents` array
- **MiniMax**: Uses XML format for tool calls `<tool_calls>...</tool_calls>`

## Basic Configuration

### OpenAI

```go
import (
    "github.com/BaSui01/agentflow/llm/providers"
    "github.com/BaSui01/agentflow/llm/providers/openai"
)

provider := openai.NewOpenAIProvider(providers.OpenAIConfig{
    APIKey:          os.Getenv("OPENAI_API_KEY"),
    Model:           "gpt-5.2",              // 2026 default model
    Organization:    "org-xxx",              // Optional: organization ID
    Timeout:         60 * time.Second,
    UseResponsesAPI: true,                   // Enable Responses API (stateful conversations)
}, logger)

// Responses API supports stateful conversations
ctx := context.WithValue(ctx, "previous_response_id", "resp_xxx")
response, _ := provider.Completion(ctx, req)
```

### Anthropic Claude

```go
import (
    "github.com/BaSui01/agentflow/llm/providers"
    "github.com/BaSui01/agentflow/llm/providers/anthropic"
)

provider := anthropic.NewClaudeProvider(providers.ClaudeConfig{
    APIKey:  os.Getenv("ANTHROPIC_API_KEY"),
    Model:   "claude-opus-4.5-20260105",     // 2026 default model
    Timeout: 120 * time.Second,              // Claude responds slower, recommend 120s
}, logger)

// Claude-specific features: hybrid reasoning mode, Thought Signatures
req := &llm.ChatRequest{
    Messages:          messages,
    ReasoningMode:     "extended",           // 2026: fast/extended
    ThoughtSignatures: []string{"sig1"},     // 2026: Thought Signatures
}
```

### Google Gemini

```go
import (
    "github.com/BaSui01/agentflow/llm/providers"
    "github.com/BaSui01/agentflow/llm/providers/gemini"
)

provider := gemini.NewGeminiProvider(providers.GeminiConfig{
    APIKey:  os.Getenv("GEMINI_API_KEY"),
    Model:   "gemini-3-pro",                 // 2026 default model
    Timeout: 60 * time.Second,
}, logger)

// Gemini supports multimodal (images, audio, video)
// Message format automatically converted to Gemini's contents array
```

### DeepSeek

```go
import (
    "github.com/BaSui01/agentflow/llm/providers"
    "github.com/BaSui01/agentflow/llm/providers/deepseek"
)

provider := deepseek.NewDeepSeekProvider(providers.DeepSeekConfig{
    APIKey:  os.Getenv("DEEPSEEK_API_KEY"),
    Model:   "deepseek-chat",
    Timeout: 60 * time.Second,
}, logger)

// DeepSeek reasoning mode: auto-switches to deepseek-reasoner
req := &llm.ChatRequest{
    Messages:      messages,
    ReasoningMode: "thinking",               // Auto-uses deepseek-reasoner
}
```

### Chinese LLM Providers

```go
// Alibaba Qwen (DashScope API)
qwenProvider := qwen.NewQwenProvider(providers.QwenConfig{
    APIKey:  os.Getenv("QWEN_API_KEY"),
    BaseURL: "https://dashscope.aliyuncs.com",  // Default
    Model:   "qwen3-235b-a22b",                 // 2026 default
}, logger)

// Zhipu GLM
glmProvider := glm.NewGLMProvider(providers.GLMConfig{
    APIKey:  os.Getenv("GLM_API_KEY"),
    BaseURL: "https://open.bigmodel.cn",        // Default
    Model:   "glm-4-plus",
}, logger)

// Tencent Hunyuan (OpenAI compatible)
hunyuanProvider := hunyuan.NewHunyuanProvider(providers.HunyuanConfig{
    APIKey:  os.Getenv("HUNYUAN_API_KEY"),
    BaseURL: "https://api.hunyuan.cloud.tencent.com/v1",
    Model:   "hunyuan-pro",
}, logger)

// ByteDance Doubao (Volcano Engine)
doubaoProvider := doubao.NewDoubaoProvider(providers.DoubaoConfig{
    APIKey:  os.Getenv("DOUBAO_API_KEY"),
    BaseURL: "https://ark.cn-beijing.volces.com",  // Default
    Model:   "Doubao-1.5-pro-32k",
}, logger)

// Moonshot Kimi (OpenAI compatible)
kimiProvider := kimi.NewKimiProvider(providers.KimiConfig{
    APIKey:  os.Getenv("KIMI_API_KEY"),
    BaseURL: "https://api.moonshot.cn",            // Default
    Model:   "moonshot-v1-128k",
}, logger)
```

### Other Providers

```go
// xAI Grok
grokProvider := grok.NewGrokProvider(providers.GrokConfig{
    APIKey:  os.Getenv("GROK_API_KEY"),
    BaseURL: "https://api.x.ai",                   // Default
    Model:   "grok-beta",
}, logger)

// Mistral AI (EU compliance)
mistralProvider := mistral.NewMistralProvider(providers.MistralConfig{
    APIKey:  os.Getenv("MISTRAL_API_KEY"),
    BaseURL: "https://api.mistral.ai",             // Default
    Model:   "mistral-large",
}, logger)

// MiniMax (XML tool call format)
minimaxProvider := minimax.NewMiniMaxProvider(providers.MiniMaxConfig{
    APIKey:  os.Getenv("MINIMAX_API_KEY"),
    BaseURL: "https://api.minimax.io",             // Default
    Model:   "abab6.5s-chat",
}, logger)

// Meta Llama (multi-platform hosting)
llamaProvider := llama.NewLlamaProvider(providers.LlamaConfig{
    APIKey:   os.Getenv("TOGETHER_API_KEY"),
    Provider: "together",                          // together/replicate/openrouter
    Model:    "meta-llama/Llama-3.3-70B-Instruct-Turbo",
}, logger)
```

## Custom BaseURL

Support for proxies, private deployments, and compatible APIs:

```go
// OpenAI compatible API (Azure, local deployment, proxy)
provider := openai.NewOpenAIProvider(providers.OpenAIConfig{
    APIKey:  os.Getenv("API_KEY"),
    BaseURL: "https://your-proxy.com/v1",
    Model:   "gpt-4o",
}, logger)

// Llama multi-platform hosting
llamaProvider := llama.NewLlamaProvider(providers.LlamaConfig{
    APIKey:   os.Getenv("TOGETHER_API_KEY"),
    Provider: "together",  // together/replicate/openrouter
    Model:    "meta-llama/Llama-3.3-70B-Instruct-Turbo",
}, logger)

// Auto-selects BaseURL
// together  -> https://api.together.xyz
// replicate -> https://api.replicate.com
// openrouter -> https://openrouter.ai/api
```

## Provider Interface

All providers implement a unified interface:

```go
type Provider interface {
    // Name returns the provider name
    Name() string
    
    // Completion synchronous completion request
    Completion(ctx context.Context, req *ChatRequest) (*ChatResponse, error)
    
    // Stream streaming completion request
    Stream(ctx context.Context, req *ChatRequest) (<-chan StreamChunk, error)
    
    // HealthCheck health check
    HealthCheck(ctx context.Context) (*HealthStatus, error)
    
    // SupportsNativeFunctionCalling whether native tool calling is supported
    SupportsNativeFunctionCalling() bool
}
```

## Credential Override

Dynamically switch API keys at runtime:

```go
// Override credentials from context
ctx := llm.ContextWithCredentialOverride(ctx, &llm.CredentialOverride{
    APIKey: "sk-dynamic-key",
})

// Provider automatically uses overridden credentials
response, err := provider.Completion(ctx, req)
```

## Resilient Provider

`ResilientProvider` wraps the original provider, providing:
- Automatic retry (exponential backoff)
- Circuit breaker (prevent cascading failures)
- Idempotency guarantee (prevent duplicate requests)

```go
import "github.com/BaSui01/agentflow/llm"

// Create idempotency manager
idempotencyMgr := llm.NewIdempotencyManager(llm.IdempotencyConfig{
    TTL:      5 * time.Minute,
    MaxSize:  10000,
})

// Wrap as resilient provider
resilientProvider := llm.NewResilientProviderSimple(
    provider,
    idempotencyMgr,
    logger,
)

// Use resilient provider
response, err := resilientProvider.Completion(ctx, request)
```

### Custom Retry Strategy

```go
resilientProvider := llm.NewResilientProvider(
    provider,
    idempotencyMgr,
    llm.ResilientConfig{
        MaxRetries:      5,
        InitialDelay:    100 * time.Millisecond,
        MaxDelay:        30 * time.Second,
        BackoffFactor:   2.0,
        RetryableErrors: []string{"rate_limit", "timeout", "server_error"},
    },
    logger,
)
```

## Multi-Provider Routing (DB-backed)

`llm.MultiProviderRouter` routes the *same model name* across multiple providers using a database-backed catalog (`sc_llm_*`) and per-provider API key pools.

### Strategies

| Strategy | Description |
|----------|-------------|
| `StrategyCostBased` | Prefer the lowest (input+output) price |
| `StrategyHealthBased` | Prefer highest health score (tie-break by priority) |
| `StrategyQPSBased` | Prefer lowest current QPS (tie-break by priority) |

### Minimal runnable example

```go
package main

import (
    "context"
    "fmt"
    "os"

    "github.com/BaSui01/agentflow/llm"
    "github.com/BaSui01/agentflow/llm/providers"
    openaiprov "github.com/BaSui01/agentflow/llm/providers/openai"
    "github.com/glebarez/sqlite"
    "go.uber.org/zap"
    "gorm.io/gorm"
)

func main() {
    logger, _ := zap.NewDevelopment()
    defer logger.Sync()

    ctx := context.Background()

    db, err := gorm.Open(sqlite.Open("file::memory:?cache=shared"), &gorm.Config{})
    if err != nil {
        panic(err)
    }
    if err := llm.InitDatabase(db); err != nil {
        panic(err)
    }

    provider := llm.LLMProvider{Code: "openai", Name: "OpenAI", Status: llm.LLMProviderStatusActive}
    if err := db.Create(&provider).Error; err != nil {
        panic(err)
    }
    model := llm.LLMModel{ModelName: "gpt-4o", DisplayName: "GPT-4o", Enabled: true}
    if err := db.Create(&model).Error; err != nil {
        panic(err)
    }
    if err := db.Create(&llm.LLMProviderModel{
        ModelID:         model.ID,
        ProviderID:      provider.ID,
        RemoteModelName: "gpt-4o",
        BaseURL:         "https://api.openai.com",
        PriceInput:      0.001,
        PriceCompletion: 0.002,
        Priority:        10,
        Enabled:         true,
    }).Error; err != nil {
        panic(err)
    }

    apiKey := os.Getenv("OPENAI_API_KEY")
    if apiKey == "" {
        apiKey = "sk-xxx" // demo key (no live call without real key)
    }
    if err := db.Create(&llm.LLMProviderAPIKey{
        ProviderID: provider.ID,
        APIKey:     apiKey,
        Label:      "default",
        Priority:   10,
        Weight:     100,
        Enabled:    true,
    }).Error; err != nil {
        panic(err)
    }

    factory := llm.NewDefaultProviderFactory()
    factory.RegisterProvider("openai", func(apiKey, baseURL string) (llm.Provider, error) {
        return openaiprov.NewOpenAIProvider(providers.OpenAIConfig{
            APIKey:  apiKey,
            BaseURL: baseURL,
        }, logger), nil
    })

    router := llm.NewMultiProviderRouter(db, factory, llm.RouterOptions{Logger: logger})
    if err := router.InitAPIKeyPools(ctx); err != nil {
        panic(err)
    }

    sel, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyCostBased)
    if err != nil {
        panic(err)
    }

    fmt.Printf("selected provider=%s model=%s\n", sel.ProviderCode, sel.ModelName)
}
```

## API Key Pooling

`llm.APIKeyPool` is a per-provider, database-backed API key pool. `MultiProviderRouter.InitAPIKeyPools(...)` creates and loads pools automatically, but you can use it directly if needed:

```go
pool := llm.NewAPIKeyPool(db, providerID, llm.StrategyWeightedRandom, logger)
_ = pool.LoadKeys(ctx)

key, err := pool.SelectKey(ctx)
if err != nil {
    // handle err
}

// Record success/failure (updates DB asynchronously)
_ = pool.RecordSuccess(ctx, key.ID)
```

## Streaming Response

All providers support streaming via `Provider.Stream`:

```go
stream, err := provider.Stream(ctx, &llm.ChatRequest{
    Model: "gpt-4o",
    Messages: []llm.Message{
        {Role: llm.RoleUser, Content: "Write a poem"},
    },
})
if err != nil {
    log.Fatal(err)
}

for chunk := range stream {
    if chunk.Err != nil {
        log.Printf("Error: %v", chunk.Err)
        break
    }
    fmt.Print(chunk.Delta.Content)
}
```

## Environment Variables

Recommended to use environment variables for sensitive information:

```bash
# OpenAI
export OPENAI_API_KEY="sk-..."

# Anthropic
export ANTHROPIC_API_KEY="sk-ant-..."

# Google
export GEMINI_API_KEY="..."

# Chinese LLM Providers
export DEEPSEEK_API_KEY="sk-..."
export QWEN_API_KEY="sk-..."
export GLM_API_KEY="..."
export HUNYUAN_API_KEY="..."
export DOUBAO_API_KEY="..."
export KIMI_API_KEY="sk-..."
```

## Cost Tracking

Built-in cost tracking functionality:

```go
import "github.com/BaSui01/agentflow/llm/observability"

tracker := observability.NewCostTracker(observability.CostConfig{
    Pricing: map[string]observability.ModelPricing{
        "gpt-4o": {
            InputPer1K:  0.005,
            OutputPer1K: 0.015,
        },
        "claude-4-opus": {
            InputPer1K:  0.015,
            OutputPer1K: 0.075,
        },
    },
})

// Record usage
tracker.RecordUsage("gpt-4o", usage.PromptTokens, usage.CompletionTokens)

// Get statistics
stats := tracker.GetStats()
fmt.Printf("Total cost: $%.4f\n", stats.TotalCost)
```


## Token Budget Management

Control token usage and costs:

```go
import "github.com/BaSui01/agentflow/llm/budget"

// Create budget manager
budgetMgr := budget.NewTokenBudgetManager(budget.BudgetConfig{
    MaxTokensPerRequest: 100000,    // Max tokens per request
    MaxTokensPerMinute:  500000,    // Max tokens per minute
    MaxTokensPerHour:    5000000,   // Max tokens per hour
    MaxTokensPerDay:     50000000,  // Max tokens per day
    MaxCostPerRequest:   10.0,      // Max cost per request
    MaxCostPerDay:       1000.0,    // Max cost per day
    AlertThreshold:      0.8,       // Alert at 80%
    AutoThrottle:        true,      // Auto throttle
    ThrottleDelay:       time.Second,
}, logger)

// Register alert handler
budgetMgr.OnAlert(func(alert budget.Alert) {
    log.Printf("Budget alert: %s, current usage: %.2f%%", alert.Message, alert.Current*100)
    // Send notification...
})

// Check budget before request
err := budgetMgr.CheckBudget(ctx, estimatedTokens, estimatedCost)
if err != nil {
    log.Printf("Budget exceeded: %v", err)
    return
}

// Record usage after request
budgetMgr.RecordUsage(budget.UsageRecord{
    Timestamp: time.Now(),
    Tokens:    response.Usage.TotalTokens,
    Cost:      calculateCost(response.Usage),
    Model:     "gpt-4o",
    RequestID: requestID,
})

// Get current status
status := budgetMgr.GetStatus()
fmt.Printf("Today's token usage: %d (%.1f%%)\n", 
    status.TokensUsedDay, status.DayUtilization*100)
fmt.Printf("Today's cost: $%.2f (%.1f%%)\n", 
    status.CostUsedDay, status.CostUtilization*100)
```

## Context Management

AgentFlow provides context engineering via `agent/context.Engineer` and `agent/context.AgentContextManager`, which automatically compresses/truncates message history to fit the model context window.

```go
import (
    "context"
    "fmt"

    agentcontext "github.com/BaSui01/agentflow/agent/context"
    "github.com/BaSui01/agentflow/types"
)

// Create context engineer (uses estimate tokenizer by default)
engineer := agentcontext.New(agentcontext.DefaultConfig(), logger)

messages := []types.Message{
    {Role: types.RoleSystem, Content: "You are an assistant"},
    {Role: types.RoleUser, Content: "Question 1..."},
    {Role: types.RoleAssistant, Content: "Answer 1..."},
    // ... more messages
}

status := engineer.GetStatus(messages)
fmt.Printf("tokens=%d usage=%.2f%% recommendation=%s\n",
    status.CurrentTokens, status.UsageRatio*100, status.Recommendation)

// Ensure messages fit (auto compress/truncate when necessary)
trimmed, err := engineer.MustFit(context.Background(), messages, "current query")
```

### Agent Integration

```go
cfg := agentcontext.DefaultAgentContextConfig("gpt-4o")
mgr := agentcontext.NewAgentContextManager(cfg, logger)

prepared, err := mgr.PrepareMessages(ctx, messages, currentQuery)
```

> Note: Fine-grained pruning strategies and LLM-based summarization are planned, but are not exposed as a standalone `PruneByStrategy` API in the current version.

## Backpressure Streaming

Streaming response handling for high-throughput scenarios:

```go
import "github.com/BaSui01/agentflow/llm/streaming"

// Create backpressure stream
stream := streaming.NewBackpressureStream(streaming.BackpressureConfig{
    BufferSize:      1024,           // Buffer size
    HighWaterMark:   0.8,            // Pause production at 80%
    LowWaterMark:    0.2,            // Resume production at 20%
    SlowConsumerTTL: 30 * time.Second,
    DropPolicy:      streaming.DropPolicyBlock, // Block policy
})

// Producer (write tokens)
go func() {
    for token := range llmStream {
        err := stream.Write(ctx, streaming.Token{
            Content:   token.Delta.Content,
            Index:     token.Index,
            Timestamp: time.Now(),
        })
        if err != nil {
            log.Printf("Write failed: %v", err)
            break
        }
    }
    stream.Close()
}()

// Consumer (read tokens)
for {
    token, err := stream.Read(ctx)
    if err == streaming.ErrStreamClosed {
        break
    }
    if err != nil {
        log.Printf("Read failed: %v", err)
        break
    }
    fmt.Print(token.Content)
}

// View statistics
stats := stream.Stats()
fmt.Printf("Produced: %d, Consumed: %d, Dropped: %d\n", 
    stats.Produced, stats.Consumed, stats.Dropped)
```

### Drop Policies

| Policy | Description |
|--------|-------------|
| `DropPolicyBlock` | Block producer until buffer has space |
| `DropPolicyOldest` | Drop oldest tokens |
| `DropPolicyNewest` | Drop newest tokens |
| `DropPolicyError` | Return error |

### Stream Multiplexing

```go
// Create multiplexer
multiplexer := streaming.NewStreamMultiplexer(sourceStream)

// Add multiple consumers
consumer1 := multiplexer.AddConsumer(streaming.DefaultBackpressureConfig())
consumer2 := multiplexer.AddConsumer(streaming.DefaultBackpressureConfig())

// Start multiplexing
multiplexer.Start(ctx)

// Each consumer consumes independently
go processStream(consumer1)
go processStream(consumer2)
```

## Advanced Routing Strategies

### Cost-Optimized Routing

```go
router := llm.NewMultiProviderRouter(db, providerFactory, llm.RouterOptions{
    Logger:              logger,
    HealthCheckInterval: 30 * time.Second,
    HealthCheckTimeout:  10 * time.Second,
})

// Initialize API key pools
if err := router.InitAPIKeyPools(ctx); err != nil {
    // handle err
}

// Select cheapest healthy provider
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyCostBased)
```

### Health-Based Routing

```go
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyHealthBased)
```

### QPS Load Balancing

```go
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyQPSBased)
```

## Best Practices

1. **Use ResilientProvider in production**: Provides retry and circuit breaker protection
2. **Multi-provider routing**: Avoid single points of failure, improve availability
3. **API key pooling**: Improve concurrency, distribute rate limiting risk
4. **Environment variables**: Never hardcode API keys in code
5. **Cost monitoring**: Track token usage and costs
6. **Health checks**: Regularly check provider availability
7. **Token budgets**: Set budget limits to prevent cost overruns
8. **Context management**: Properly trim messages to avoid exceeding limits
9. **Backpressure handling**: Use backpressure streams for high-throughput scenarios
