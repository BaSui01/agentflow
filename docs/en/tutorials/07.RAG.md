# Retrieval-Augmented Generation (RAG)

AgentFlow provides a complete RAG implementation with hybrid retrieval, vector stores, reranking, and context engineering.

## Hybrid Retrieval

Combines BM25 and vector search:

```go
import "github.com/BaSui01/agentflow/rag"

retriever := rag.NewHybridRetriever(rag.HybridRetrievalConfig{
    UseBM25:      true,
    BM25Weight:   0.5,
    UseVector:    true,
    VectorWeight: 0.5,
    UseReranking: true,
    RerankTopK:   50,
    TopK:         5,
    MinScore:     0.3,
}, logger)

// Index documents
retriever.IndexDocuments(docs)

// Retrieve
results, err := retriever.Retrieve(ctx, query, queryEmbedding)

for _, r := range results {
    fmt.Printf("Doc: %s, Score: %.3f\n", r.Document.ID, r.FinalScore)
}
```

## Vector Stores

**Supported backends**

| Backend | Status | Notes |
|--------|--------|------|
| In-memory | ✅ Supported | Suitable for tests / small datasets |
| Qdrant | ✅ Supported | REST API client (`AutoCreateCollection` optional) |
| Pinecone | ✅ Supported | REST API client (can auto-resolve host via controller API) |

```go
import "github.com/BaSui01/agentflow/rag"

// In-memory
vectorStore := rag.NewInMemoryVectorStore(logger)

// Qdrant (REST)
qdrantStore := rag.NewQdrantStore(rag.QdrantConfig{
    Host:                 "localhost",
    Port:                 6333,
    Collection:           "documents",
    AutoCreateCollection: true,
}, logger)

// Pinecone (REST)
pineconeStore := rag.NewPineconeStore(rag.PineconeConfig{
    APIKey: os.Getenv("PINECONE_API_KEY"),
    Index:  "documents",
}, logger)
```

## Embedding Providers

```go
import "github.com/BaSui01/agentflow/llm/embedding"

// OpenAI
embedder := embedding.NewOpenAIProvider(embedding.OpenAIConfig{
    APIKey: os.Getenv("OPENAI_API_KEY"),
    Model:  "text-embedding-3-large",
})

vector, err := embedder.EmbedQuery(ctx, "Go concurrency")

// Batch embedding
vectors, err := embedder.EmbedDocuments(ctx, texts)

// Other providers: Cohere, Voyage, Jina, Gemini
```

## Reranking

```go
import "github.com/BaSui01/agentflow/llm/rerank"

reranker := rerank.NewCohereProvider(rerank.CohereConfig{
    APIKey: os.Getenv("COHERE_API_KEY"),
    Model:  "rerank-multilingual-v3.0",
})

reranked, err := reranker.RerankSimple(ctx, query, documents, 10)
```

## Document Chunking

```go
import "github.com/BaSui01/agentflow/rag"

chunker := rag.NewDocumentChunker(rag.DefaultChunkingConfig(), &rag.SimpleTokenizer{}, logger)
chunks := chunker.ChunkDocument(rag.Document{ID: "doc1", Content: longDocument})
```

## Context Management

```go
import (
    agentcontext "github.com/BaSui01/agentflow/agent/context"
    "github.com/BaSui01/agentflow/types"
)

engineer := agentcontext.New(agentcontext.DefaultConfig(), logger)

trimmedMsgs, err := engineer.MustFit(ctx, messages, currentQuery)
```

## Complete RAG Pipeline

```go
// 1. Create components
embedder := embedding.NewOpenAIProvider(embedding.OpenAIConfig{
    APIKey: os.Getenv("OPENAI_API_KEY"),
})
vectorStore := rag.NewInMemoryVectorStore(logger) // or Qdrant/Pinecone stores

// 2. Create retriever
retriever := rag.NewHybridRetrieverWithVectorStore(rag.DefaultHybridRetrievalConfig(), vectorStore, logger)

// 3. Index documents
contents := make([]string, len(documents))
for i := range documents {
    contents[i] = documents[i].Content
}
vectors, _ := embedder.EmbedDocuments(ctx, contents)
for i := range documents {
    documents[i].Embedding = vectors[i]
}
retriever.IndexDocuments(documents)

// 4. Retrieve
queryEmbedding, _ := embedder.EmbedQuery(ctx, query)
results, _ := retriever.Retrieve(ctx, query, queryEmbedding)

// 5. Generate answer (use retrieved documents as context)
var sb strings.Builder
for _, r := range results {
    sb.WriteString(r.Document.Content)
    sb.WriteString("\n\n")
}
context := sb.String()
response, _ := provider.Completion(ctx, &llm.ChatRequest{
    Messages: []llm.Message{
        {Role: llm.RoleUser, Content: fmt.Sprintf("Context:\n%s\n\nQuestion: %s", context, query)},
    },
})
```
