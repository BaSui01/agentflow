# Provider Configuration Guide

AgentFlow provides a unified LLM Provider abstraction layer supporting 13+ major model providers with enterprise features like resilient failover, multi-provider routing, and API key pooling.

## Supported Providers

| Provider | Default Model | Default BaseURL | Features |
|----------|---------------|-----------------|----------|
| OpenAI | gpt-5.2 | https://api.openai.com | Tool calling, multimodal, Responses API |
| Anthropic | claude-opus-4.5 | https://api.anthropic.com | Long context, chain-of-thought, Thought Signatures |
| Gemini | gemini-3-pro | https://generativelanguage.googleapis.com | Multimodal, 1M tokens context |
| DeepSeek | deepseek-chat | https://api.deepseek.com | Cost-effective, deepseek-reasoner mode |
| Qwen | qwen3-235b-a22b | https://dashscope.aliyuncs.com | Chinese optimized, DashScope API |
| GLM | glm-4-plus | https://open.bigmodel.cn | Zhipu AI, Chinese optimized |
| Grok | grok-beta | https://api.x.ai | xAI, real-time info |
| MiniMax | abab6.5s-chat | https://api.minimax.io | XML tool call format |
| Mistral | mistral-large | https://api.mistral.ai | EU compliance, OpenAI compatible |
| Hunyuan | hunyuan-pro | https://api.hunyuan.cloud.tencent.com/v1 | Tencent, OpenAI compatible |
| Kimi | moonshot-v1 | https://api.moonshot.cn | Long context, OpenAI compatible |
| Llama | llama-3.3-70b | https://api.together.xyz | Multi-platform hosting (Together/Replicate/OpenRouter) |
| Doubao | Doubao-1.5-pro-32k | https://ark.cn-beijing.volces.com | ByteDance Volcano Engine |

## API Format Classification

### OpenAI Compatible API
The following providers use OpenAI compatible API, sharing the same request/response format:
- OpenAI, DeepSeek, Qwen, GLM, Grok, Mistral, Hunyuan, Kimi, Llama, Doubao

### Custom API
- **Anthropic Claude**: Uses `x-api-key` authentication, system message passed separately, different SSE streaming format
- **Google Gemini**: Uses `x-goog-api-key` authentication, message format uses `contents` array
- **MiniMax**: Uses XML format for tool calls `<tool_calls>...</tool_calls>`

## Basic Configuration

### OpenAI

```go
import (
    "github.com/BaSui01/agentflow/providers/openai"
    "github.com/BaSui01/agentflow/providers"
)

provider := openai.NewOpenAIProvider(providers.OpenAIConfig{
    APIKey:          os.Getenv("OPENAI_API_KEY"),
    Model:           "gpt-5.2",              // 2026 default model
    Organization:    "org-xxx",              // Optional: organization ID
    Timeout:         60 * time.Second,
    UseResponsesAPI: true,                   // Enable Responses API (stateful conversations)
}, logger)

// Responses API supports stateful conversations
ctx := context.WithValue(ctx, "previous_response_id", "resp_xxx")
response, _ := provider.Completion(ctx, req)
```

### Anthropic Claude

```go
import "github.com/BaSui01/agentflow/providers/anthropic"

provider := anthropic.NewClaudeProvider(providers.ClaudeConfig{
    APIKey:  os.Getenv("ANTHROPIC_API_KEY"),
    Model:   "claude-opus-4.5-20260105",     // 2026 default model
    Timeout: 120 * time.Second,              // Claude responds slower, recommend 120s
}, logger)

// Claude-specific features: hybrid reasoning mode, Thought Signatures
req := &llm.ChatRequest{
    Messages:          messages,
    ReasoningMode:     "extended",           // 2026: fast/extended
    ThoughtSignatures: []string{"sig1"},     // 2026: Thought Signatures
}
```

### Google Gemini

```go
import "github.com/BaSui01/agentflow/providers/gemini"

provider := gemini.NewGeminiProvider(providers.GeminiConfig{
    APIKey:  os.Getenv("GEMINI_API_KEY"),
    Model:   "gemini-3-pro",                 // 2026 default model
    Timeout: 60 * time.Second,
}, logger)

// Gemini supports multimodal (images, audio, video)
// Message format automatically converted to Gemini's contents array
```

### DeepSeek

```go
import "github.com/BaSui01/agentflow/providers/deepseek"

provider := deepseek.NewDeepSeekProvider(providers.DeepSeekConfig{
    APIKey:  os.Getenv("DEEPSEEK_API_KEY"),
    Model:   "deepseek-chat",
    Timeout: 60 * time.Second,
}, logger)

// DeepSeek reasoning mode: auto-switches to deepseek-reasoner
req := &llm.ChatRequest{
    Messages:      messages,
    ReasoningMode: "thinking",               // Auto-uses deepseek-reasoner
}
```

### Chinese LLM Providers

```go
// Alibaba Qwen (DashScope API)
qwenProvider := qwen.NewQwenProvider(providers.QwenConfig{
    APIKey:  os.Getenv("QWEN_API_KEY"),
    BaseURL: "https://dashscope.aliyuncs.com",  // Default
    Model:   "qwen3-235b-a22b",                 // 2026 default
}, logger)

// Zhipu GLM
glmProvider := glm.NewGLMProvider(providers.GLMConfig{
    APIKey:  os.Getenv("GLM_API_KEY"),
    BaseURL: "https://open.bigmodel.cn",        // Default
    Model:   "glm-4-plus",
}, logger)

// Tencent Hunyuan (OpenAI compatible)
hunyuanProvider := hunyuan.NewHunyuanProvider(providers.HunyuanConfig{
    APIKey:  os.Getenv("HUNYUAN_API_KEY"),
    BaseURL: "https://api.hunyuan.cloud.tencent.com/v1",
    Model:   "hunyuan-pro",
}, logger)

// ByteDance Doubao (Volcano Engine)
doubaoProvider := doubao.NewDoubaoProvider(providers.DoubaoConfig{
    APIKey:  os.Getenv("DOUBAO_API_KEY"),
    BaseURL: "https://ark.cn-beijing.volces.com",  // Default
    Model:   "Doubao-1.5-pro-32k",
}, logger)

// Moonshot Kimi (OpenAI compatible)
kimiProvider := kimi.NewKimiProvider(providers.KimiConfig{
    APIKey:  os.Getenv("KIMI_API_KEY"),
    BaseURL: "https://api.moonshot.cn",            // Default
    Model:   "moonshot-v1-128k",
}, logger)
```

### Other Providers

```go
// xAI Grok
grokProvider := grok.NewGrokProvider(providers.GrokConfig{
    APIKey:  os.Getenv("GROK_API_KEY"),
    BaseURL: "https://api.x.ai",                   // Default
    Model:   "grok-beta",
}, logger)

// Mistral AI (EU compliance)
mistralProvider := mistral.NewMistralProvider(providers.MistralConfig{
    APIKey:  os.Getenv("MISTRAL_API_KEY"),
    BaseURL: "https://api.mistral.ai",             // Default
    Model:   "mistral-large",
}, logger)

// MiniMax (XML tool call format)
minimaxProvider := minimax.NewMiniMaxProvider(providers.MiniMaxConfig{
    APIKey:  os.Getenv("MINIMAX_API_KEY"),
    BaseURL: "https://api.minimax.io",             // Default
    Model:   "abab6.5s-chat",
}, logger)

// Meta Llama (multi-platform hosting)
llamaProvider := llama.NewLlamaProvider(providers.LlamaConfig{
    APIKey:   os.Getenv("TOGETHER_API_KEY"),
    Provider: "together",                          // together/replicate/openrouter
    Model:    "meta-llama/Llama-3.3-70B-Instruct-Turbo",
}, logger)
```

## Custom BaseURL

Support for proxies, private deployments, and compatible APIs:

```go
// OpenAI compatible API (Azure, local deployment, proxy)
provider := openai.NewOpenAIProvider(providers.OpenAIConfig{
    APIKey:  os.Getenv("API_KEY"),
    BaseURL: "https://your-proxy.com/v1",
    Model:   "gpt-4o",
}, logger)

// Llama multi-platform hosting
llamaProvider := llama.NewLlamaProvider(providers.LlamaConfig{
    APIKey:   os.Getenv("TOGETHER_API_KEY"),
    Provider: "together",  // together/replicate/openrouter
    Model:    "meta-llama/Llama-3.3-70B-Instruct-Turbo",
}, logger)

// Auto-selects BaseURL
// together  -> https://api.together.xyz
// replicate -> https://api.replicate.com
// openrouter -> https://openrouter.ai/api
```

## Provider Interface

All providers implement a unified interface:

```go
type Provider interface {
    // Name returns the provider name
    Name() string
    
    // Completion synchronous completion request
    Completion(ctx context.Context, req *ChatRequest) (*ChatResponse, error)
    
    // Stream streaming completion request
    Stream(ctx context.Context, req *ChatRequest) (<-chan StreamChunk, error)
    
    // HealthCheck health check
    HealthCheck(ctx context.Context) (*HealthStatus, error)
    
    // SupportsNativeFunctionCalling whether native tool calling is supported
    SupportsNativeFunctionCalling() bool
}
```

## Credential Override

Dynamically switch API keys at runtime:

```go
// Override credentials from context
ctx := llm.ContextWithCredentialOverride(ctx, &llm.CredentialOverride{
    APIKey: "sk-dynamic-key",
})

// Provider automatically uses overridden credentials
response, err := provider.Completion(ctx, req)
```

## Resilient Provider

`ResilientProvider` wraps the original provider, providing:
- Automatic retry (exponential backoff)
- Circuit breaker (prevent cascading failures)
- Idempotency guarantee (prevent duplicate requests)

```go
import "github.com/BaSui01/agentflow/llm"

// Create idempotency manager
idempotencyMgr := llm.NewIdempotencyManager(llm.IdempotencyConfig{
    TTL:      5 * time.Minute,
    MaxSize:  10000,
})

// Wrap as resilient provider
resilientProvider := llm.NewResilientProviderSimple(
    provider,
    idempotencyMgr,
    logger,
)

// Use resilient provider
response, err := resilientProvider.Completion(ctx, request)
```

### Custom Retry Strategy

```go
resilientProvider := llm.NewResilientProvider(
    provider,
    idempotencyMgr,
    llm.ResilientConfig{
        MaxRetries:      5,
        InitialDelay:    100 * time.Millisecond,
        MaxDelay:        30 * time.Second,
        BackoffFactor:   2.0,
        RetryableErrors: []string{"rate_limit", "timeout", "server_error"},
    },
    logger,
)
```

## Multi-Provider Router

`MultiProviderRouter` supports multiple routing strategies:

```go
router := llm.NewMultiProviderRouter([]llm.Provider{
    openaiProvider,
    claudeProvider,
    geminiProvider,
}, llm.RouterConfig{
    Strategy:       llm.StrategyFallback,    // Failover
    HealthCheck:    true,                     // Health check
    CheckInterval:  30 * time.Second,
    Timeout:        60 * time.Second,
}, logger)
```

### Routing Strategies

| Strategy | Description |
|----------|-------------|
| `StrategyFallback` | Failover: switch to backup when primary fails |
| `StrategyRoundRobin` | Round-robin: distribute requests evenly |
| `StrategyWeighted` | Weighted: distribute by weight |
| `StrategyLowestLatency` | Lowest latency: select fastest provider |
| `StrategyCostOptimized` | Cost optimized: select cheapest provider |

### Weighted Routing Example

```go
router := llm.NewMultiProviderRouter(providers, llm.RouterConfig{
    Strategy: llm.StrategyWeighted,
    Weights: map[string]float64{
        "openai":    0.5,
        "anthropic": 0.3,
        "gemini":    0.2,
    },
}, logger)
```

## API Key Pooling

Support for multiple API key rotation, improving concurrency and availability:

```go
pool := llm.NewAPIKeyPool([]string{
    os.Getenv("OPENAI_API_KEY_1"),
    os.Getenv("OPENAI_API_KEY_2"),
    os.Getenv("OPENAI_API_KEY_3"),
}, llm.APIKeyPoolConfig{
    Strategy:        llm.KeyStrategyRoundRobin,
    HealthCheck:     true,
    CheckInterval:   60 * time.Second,
    MaxFailures:     3,
    CooldownPeriod:  5 * time.Minute,
})

// Get available key
key, err := pool.GetKey(ctx)
```

## Streaming Response

All providers support streaming responses:

```go
stream, err := provider.CompletionStream(ctx, &llm.ChatRequest{
    Model: "gpt-4o",
    Messages: []llm.Message{
        {Role: llm.RoleUser, Content: "Write a poem"},
    },
})
if err != nil {
    log.Fatal(err)
}

for chunk := range stream {
    if chunk.Error != nil {
        log.Printf("Error: %v", chunk.Error)
        break
    }
    fmt.Print(chunk.Delta.Content)
}
```

## Environment Variables

Recommended to use environment variables for sensitive information:

```bash
# OpenAI
export OPENAI_API_KEY="sk-..."

# Anthropic
export ANTHROPIC_API_KEY="sk-ant-..."

# Google
export GEMINI_API_KEY="..."

# Chinese LLM Providers
export DEEPSEEK_API_KEY="sk-..."
export QWEN_API_KEY="sk-..."
export GLM_API_KEY="..."
export HUNYUAN_API_KEY="..."
export DOUBAO_API_KEY="..."
export KIMI_API_KEY="sk-..."
```

## Cost Tracking

Built-in cost tracking functionality:

```go
import "github.com/BaSui01/agentflow/llm/observability"

tracker := observability.NewCostTracker(observability.CostConfig{
    Pricing: map[string]observability.ModelPricing{
        "gpt-4o": {
            InputPer1K:  0.005,
            OutputPer1K: 0.015,
        },
        "claude-4-opus": {
            InputPer1K:  0.015,
            OutputPer1K: 0.075,
        },
    },
})

// Record usage
tracker.RecordUsage("gpt-4o", usage.PromptTokens, usage.CompletionTokens)

// Get statistics
stats := tracker.GetStats()
fmt.Printf("Total cost: $%.4f\n", stats.TotalCost)
```


## Token Budget Management

Control token usage and costs:

```go
import "github.com/BaSui01/agentflow/llm/budget"

// Create budget manager
budgetMgr := budget.NewTokenBudgetManager(budget.BudgetConfig{
    MaxTokensPerRequest: 100000,    // Max tokens per request
    MaxTokensPerMinute:  500000,    // Max tokens per minute
    MaxTokensPerHour:    5000000,   // Max tokens per hour
    MaxTokensPerDay:     50000000,  // Max tokens per day
    MaxCostPerRequest:   10.0,      // Max cost per request
    MaxCostPerDay:       1000.0,    // Max cost per day
    AlertThreshold:      0.8,       // Alert at 80%
    AutoThrottle:        true,      // Auto throttle
    ThrottleDelay:       time.Second,
}, logger)

// Register alert handler
budgetMgr.OnAlert(func(alert budget.Alert) {
    log.Printf("Budget alert: %s, current usage: %.2f%%", alert.Message, alert.Current*100)
    // Send notification...
})

// Check budget before request
err := budgetMgr.CheckBudget(ctx, estimatedTokens, estimatedCost)
if err != nil {
    log.Printf("Budget exceeded: %v", err)
    return
}

// Record usage after request
budgetMgr.RecordUsage(budget.UsageRecord{
    Timestamp: time.Now(),
    Tokens:    response.Usage.TotalTokens,
    Cost:      calculateCost(response.Usage),
    Model:     "gpt-4o",
    RequestID: requestID,
})

// Get current status
status := budgetMgr.GetStatus()
fmt.Printf("Today's token usage: %d (%.1f%%)\n", 
    status.TokensUsedDay, status.DayUtilization*100)
fmt.Printf("Today's cost: $%.2f (%.1f%%)\n", 
    status.CostUsedDay, status.CostUtilization*100)
```

## Context Management

Intelligent conversation context management to avoid exceeding token limits:

```go
import "github.com/BaSui01/agentflow/llm/context"

// Create context manager
tokenizer := context.NewSimpleTokenizer() // Or use tiktoken
ctxMgr := context.NewDefaultContextManager(tokenizer, logger)

// Trim messages to fit token limit
messages := []context.Message{
    {Role: context.RoleSystem, Content: "You are an assistant"},
    {Role: context.RoleUser, Content: "Question 1..."},
    {Role: context.RoleAssistant, Content: "Answer 1..."},
    // ... more messages
}

// Trim using different strategies
trimmed, err := ctxMgr.PruneByStrategy(messages, 4096, context.PruneOldest)
```

### Pruning Strategies

| Strategy | Description |
|----------|-------------|
| `PruneOldest` | Remove oldest messages, keep System and recent messages |
| `PruneByRole` | Prune by role priority (System > User/Assistant > Tool) |
| `PruneLeastImportant` | Prune by message importance (requires Metadata.importance) |
| `PruneSlidingWindow` | Sliding window, keep most recent N messages |
| `PruneToolCalls` | Prioritize removing tool calls and results |

### Context Management with Compression

```go
// Create summary compressor (requires LLM Provider)
compressor := context.NewSummaryCompressor(provider, context.CompressionConfig{
    Threshold:     0.8,  // Trigger compression at 80% capacity
    TargetRatio:   0.5,  // Compress to 50%
    MinMessages:   5,    // Keep at least 5 messages
})

// Create context manager with compression
enhancedMgr := context.NewDefaultContextManagerWithCompression(
    tokenizer,
    compressor,
    logger,
)

// Auto compress + trim
trimmed, err := enhancedMgr.TrimMessages(messages, 4096)
```

## Backpressure Streaming

Streaming response handling for high-throughput scenarios:

```go
import "github.com/BaSui01/agentflow/llm/streaming"

// Create backpressure stream
stream := streaming.NewBackpressureStream(streaming.BackpressureConfig{
    BufferSize:      1024,           // Buffer size
    HighWaterMark:   0.8,            // Pause production at 80%
    LowWaterMark:    0.2,            // Resume production at 20%
    SlowConsumerTTL: 30 * time.Second,
    DropPolicy:      streaming.DropPolicyBlock, // Block policy
})

// Producer (write tokens)
go func() {
    for token := range llmStream {
        err := stream.Write(ctx, streaming.Token{
            Content:   token.Delta.Content,
            Index:     token.Index,
            Timestamp: time.Now(),
        })
        if err != nil {
            log.Printf("Write failed: %v", err)
            break
        }
    }
    stream.Close()
}()

// Consumer (read tokens)
for {
    token, err := stream.Read(ctx)
    if err == streaming.ErrStreamClosed {
        break
    }
    if err != nil {
        log.Printf("Read failed: %v", err)
        break
    }
    fmt.Print(token.Content)
}

// View statistics
stats := stream.Stats()
fmt.Printf("Produced: %d, Consumed: %d, Dropped: %d\n", 
    stats.Produced, stats.Consumed, stats.Dropped)
```

### Drop Policies

| Policy | Description |
|--------|-------------|
| `DropPolicyBlock` | Block producer until buffer has space |
| `DropPolicyOldest` | Drop oldest tokens |
| `DropPolicyNewest` | Drop newest tokens |
| `DropPolicyError` | Return error |

### Stream Multiplexing

```go
// Create multiplexer
multiplexer := streaming.NewStreamMultiplexer(sourceStream)

// Add multiple consumers
consumer1 := multiplexer.AddConsumer(streaming.DefaultBackpressureConfig())
consumer2 := multiplexer.AddConsumer(streaming.DefaultBackpressureConfig())

// Start multiplexing
multiplexer.Start(ctx)

// Each consumer consumes independently
go processStream(consumer1)
go processStream(consumer2)
```

## Advanced Routing Strategies

### Cost-Optimized Routing

```go
router := llm.NewMultiProviderRouter(db, providerFactory, llm.RouterOptions{
    Strategy:      llm.StrategyCostBased,
    HealthCheck:   true,
    CheckInterval: 30 * time.Second,
})

// Initialize API key pools
router.InitAPIKeyPools(ctx)

// Select cheapest healthy provider
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyCostBased)
```

### Health-Based Routing

```go
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyHealthBased)
```

### QPS Load Balancing

```go
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyQPSBased)
```

## Best Practices

1. **Use ResilientProvider in production**: Provides retry and circuit breaker protection
2. **Multi-provider routing**: Avoid single points of failure, improve availability
3. **API key pooling**: Improve concurrency, distribute rate limiting risk
4. **Environment variables**: Never hardcode API keys in code
5. **Cost monitoring**: Track token usage and costs
6. **Health checks**: Regularly check provider availability
7. **Token budgets**: Set budget limits to prevent cost overruns
8. **Context management**: Properly trim messages to avoid exceeding limits
9. **Backpressure handling**: Use backpressure streams for high-throughput scenarios
