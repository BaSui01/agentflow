# Retrieval-Augmented Generation (RAG)

AgentFlow provides complete RAG implementation with hybrid retrieval, vector stores, reranking, and context management.

## Hybrid Retrieval

Combines BM25 and vector search:

```go
import "github.com/BaSui01/agentflow/llm/retrieval"

retriever := retrieval.NewHybridRetriever(retrieval.HybridRetrievalConfig{
    UseBM25:      true,
    BM25Weight:   0.5,
    UseVector:    true,
    VectorWeight: 0.5,
    UseReranking: true,
    RerankTopK:   50,
    TopK:         5,
    MinScore:     0.3,
}, logger)

// Index documents
retriever.IndexDocuments(docs)

// Retrieve
results, err := retriever.Retrieve(ctx, query, queryEmbedding)

for _, r := range results {
    fmt.Printf("Doc: %s, Score: %.3f\n", r.Document.ID, r.FinalScore)
}
```

## Vector Stores

```go
// Qdrant
vectorStore := retrieval.NewQdrantStore(retrieval.QdrantConfig{
    Host:       "localhost",
    Port:       6333,
    Collection: "documents",
})

// Pinecone
vectorStore := retrieval.NewPineconeStore(retrieval.PineconeConfig{
    APIKey: os.Getenv("PINECONE_API_KEY"),
    Index:  "documents",
})
```

## Embedding Providers

```go
import "github.com/BaSui01/agentflow/llm/embedding"

// OpenAI
embedder := embedding.NewOpenAIEmbedder(embedding.OpenAIConfig{
    APIKey: os.Getenv("OPENAI_API_KEY"),
    Model:  "text-embedding-3-large",
})

vector, err := embedder.Embed(ctx, "Go concurrency")

// Batch embedding
vectors, err := embedder.EmbedBatch(ctx, texts)

// Other providers: Cohere, Voyage, Jina, Gemini
```

## Reranking

```go
import "github.com/BaSui01/agentflow/llm/rerank"

reranker := rerank.NewCohereReranker(rerank.CohereConfig{
    APIKey: os.Getenv("COHERE_API_KEY"),
    Model:  "rerank-multilingual-v3.0",
})

rerankedDocs, err := reranker.Rerank(ctx, query, documents, 10)
```

## Document Chunking

```go
chunker := retrieval.NewChunker(retrieval.ChunkConfig{
    ChunkSize:    512,
    ChunkOverlap: 50,
    Separator:    "\n\n",
})

chunks := chunker.Split(longDocument)
```

## Context Management

```go
import "github.com/BaSui01/agentflow/llm/context"

manager := context.NewDefaultContextManager(tokenizer, logger)

// Trim messages to fit token limit
trimmedMsgs, err := manager.TrimMessages(messages, 4096)

// Prune by strategy
trimmedMsgs, err = manager.PruneByStrategy(messages, 4096, context.PruneOldest)
```

### Pruning Strategies

| Strategy | Description |
|----------|-------------|
| `PruneOldest` | Remove oldest, keep System and recent |
| `PruneByRole` | Prune by role priority |
| `PruneLeastImportant` | Prune by importance metadata |
| `PruneSlidingWindow` | Keep most recent N messages |
| `PruneToolCalls` | Remove tool calls first |

### Summary Compression

```go
compressor := context.NewSummaryCompressor(context.SummaryConfig{
    Provider:     provider,
    MaxTokens:    1000,
    TriggerRatio: 0.8,
})

manager := context.NewDefaultContextManagerWithCompression(tokenizer, compressor, logger)
```

## Complete RAG Pipeline

```go
// 1. Create components
embedder := embedding.NewOpenAIEmbedder(config)
vectorStore := retrieval.NewQdrantStore(config)
reranker := rerank.NewCohereReranker(config)

// 2. Create retriever
retriever := retrieval.NewHybridRetrieverWithVectorStore(config, vectorStore, logger)

// 3. Index documents
for _, doc := range documents {
    doc.Embedding, _ = embedder.Embed(ctx, doc.Content)
}
retriever.IndexDocuments(documents)

// 4. Retrieve
queryEmbedding, _ := embedder.Embed(ctx, query)
results, _ := retriever.Retrieve(ctx, query, queryEmbedding)

// 5. Rerank
rerankedResults, _ := reranker.Rerank(ctx, query, results, 5)

// 6. Generate answer
context := buildContext(rerankedResults)
response, _ := provider.Completion(ctx, &llm.ChatRequest{
    Messages: []llm.Message{
        {Role: llm.RoleUser, Content: fmt.Sprintf("Context:\n%s\n\nQuestion: %s", context, query)},
    },
})
```
