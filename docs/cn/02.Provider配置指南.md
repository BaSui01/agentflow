# Provider 配置指南

AgentFlow 提供统一的 LLM Provider 抽象层，支持 13+ 主流大模型提供商，具备弹性容错、多 Provider 路由、API Key 池化等企业级特性。

## 支持的 Provider

| Provider | 默认模型 | 默认 BaseURL | 特点 |
|----------|----------|--------------|------|
| OpenAI | gpt-5.2 | https://api.openai.com | 工具调用、多模态、Responses API |
| Anthropic | claude-opus-4.5 | https://api.anthropic.com | 长上下文、思维链、Thought Signatures |
| Gemini | gemini-3-pro | https://generativelanguage.googleapis.com | 多模态、1M tokens 上下文 |
| DeepSeek | deepseek-chat | https://api.deepseek.com | 高性价比、deepseek-reasoner 推理模式 |
| Qwen | qwen3-235b-a22b | https://dashscope.aliyuncs.com | 中文优化、DashScope API |
| GLM | glm-4-plus | https://open.bigmodel.cn | 智谱 AI、中文优化 |
| Grok | grok-beta | https://api.x.ai | xAI、实时信息 |
| MiniMax | abab6.5s-chat | https://api.minimax.io | XML 工具调用格式 |
| Mistral | mistral-large | https://api.mistral.ai | 欧洲合规、OpenAI 兼容 |
| Hunyuan | hunyuan-pro | https://api.hunyuan.cloud.tencent.com/v1 | 腾讯混元、OpenAI 兼容 |
| Kimi | moonshot-v1 | https://api.moonshot.cn | 长上下文、OpenAI 兼容 |
| Llama | llama-3.3-70b | https://api.together.xyz | 多平台托管（Together/Replicate/OpenRouter） |
| Doubao | Doubao-1.5-pro-32k | https://ark.cn-beijing.volces.com | 字节跳动火山方舟 |

## API 格式分类

### OpenAI 兼容 API
以下 Provider 使用 OpenAI 兼容 API，可复用相同的请求/响应格式：
- OpenAI、DeepSeek、Qwen、GLM、Grok、Mistral、Hunyuan、Kimi、Llama、Doubao

### 自定义 API
- **Anthropic Claude**: 使用 `x-api-key` 认证，system 消息单独传递，SSE 流式格式不同
- **Google Gemini**: 使用 `x-goog-api-key` 认证，消息格式为 `contents` 数组
- **MiniMax**: 使用 XML 格式的工具调用 `<tool_calls>...</tool_calls>`

## 基础配置

### OpenAI

```go
import (
    "github.com/BaSui01/agentflow/providers/openai"
    "github.com/BaSui01/agentflow/providers"
)

provider := openai.NewOpenAIProvider(providers.OpenAIConfig{
    APIKey:          os.Getenv("OPENAI_API_KEY"),
    Model:           "gpt-5.2",              // 2026 默认模型
    Organization:    "org-xxx",              // 可选：组织 ID
    Timeout:         60 * time.Second,
    UseResponsesAPI: true,                   // 启用 Responses API（有状态对话）
}, logger)

// Responses API 支持有状态对话
ctx := context.WithValue(ctx, "previous_response_id", "resp_xxx")
response, _ := provider.Completion(ctx, req)
```

### Anthropic Claude

```go
import "github.com/BaSui01/agentflow/providers/anthropic"

provider := anthropic.NewClaudeProvider(providers.ClaudeConfig{
    APIKey:  os.Getenv("ANTHROPIC_API_KEY"),
    Model:   "claude-opus-4.5-20260105",     // 2026 默认模型
    Timeout: 120 * time.Second,              // Claude 响应较慢，建议 120s
}, logger)

// Claude 特有功能：混合推理模式、Thought Signatures
req := &llm.ChatRequest{
    Messages:          messages,
    ReasoningMode:     "extended",           // 2026: fast/extended
    ThoughtSignatures: []string{"sig1"},     // 2026: Thought Signatures
}
```

### Google Gemini

```go
import "github.com/BaSui01/agentflow/providers/gemini"

provider := gemini.NewGeminiProvider(providers.GeminiConfig{
    APIKey:  os.Getenv("GEMINI_API_KEY"),
    Model:   "gemini-3-pro",                 // 2026 默认模型
    Timeout: 60 * time.Second,
}, logger)

// Gemini 支持多模态（图片、音频、视频）
// 消息格式自动转换为 Gemini 的 contents 数组
```

### DeepSeek

```go
import "github.com/BaSui01/agentflow/providers/deepseek"

provider := deepseek.NewDeepSeekProvider(providers.DeepSeekConfig{
    APIKey:  os.Getenv("DEEPSEEK_API_KEY"),
    Model:   "deepseek-chat",
    Timeout: 60 * time.Second,
}, logger)

// DeepSeek 推理模式：自动切换到 deepseek-reasoner
req := &llm.ChatRequest{
    Messages:      messages,
    ReasoningMode: "thinking",               // 自动使用 deepseek-reasoner
}
```

### 国产大模型

```go
// 阿里通义千问（DashScope API）
qwenProvider := qwen.NewQwenProvider(providers.QwenConfig{
    APIKey:  os.Getenv("QWEN_API_KEY"),
    BaseURL: "https://dashscope.aliyuncs.com",  // 默认
    Model:   "qwen3-235b-a22b",                 // 2026 默认
}, logger)

// 智谱 GLM
glmProvider := glm.NewGLMProvider(providers.GLMConfig{
    APIKey:  os.Getenv("GLM_API_KEY"),
    BaseURL: "https://open.bigmodel.cn",        // 默认
    Model:   "glm-4-plus",
}, logger)

// 腾讯混元（OpenAI 兼容）
hunyuanProvider := hunyuan.NewHunyuanProvider(providers.HunyuanConfig{
    APIKey:  os.Getenv("HUNYUAN_API_KEY"),
    BaseURL: "https://api.hunyuan.cloud.tencent.com/v1",
    Model:   "hunyuan-pro",
}, logger)

// 字节豆包（火山方舟）
doubaoProvider := doubao.NewDoubaoProvider(providers.DoubaoConfig{
    APIKey:  os.Getenv("DOUBAO_API_KEY"),
    BaseURL: "https://ark.cn-beijing.volces.com",  // 默认
    Model:   "Doubao-1.5-pro-32k",
}, logger)

// 月之暗面 Kimi（OpenAI 兼容）
kimiProvider := kimi.NewKimiProvider(providers.KimiConfig{
    APIKey:  os.Getenv("KIMI_API_KEY"),
    BaseURL: "https://api.moonshot.cn",            // 默认
    Model:   "moonshot-v1-128k",
}, logger)
```

### 其他 Provider

```go
// xAI Grok
grokProvider := grok.NewGrokProvider(providers.GrokConfig{
    APIKey:  os.Getenv("GROK_API_KEY"),
    BaseURL: "https://api.x.ai",                   // 默认
    Model:   "grok-beta",
}, logger)

// Mistral AI（欧洲合规）
mistralProvider := mistral.NewMistralProvider(providers.MistralConfig{
    APIKey:  os.Getenv("MISTRAL_API_KEY"),
    BaseURL: "https://api.mistral.ai",             // 默认
    Model:   "mistral-large",
}, logger)

// MiniMax（XML 工具调用格式）
minimaxProvider := minimax.NewMiniMaxProvider(providers.MiniMaxConfig{
    APIKey:  os.Getenv("MINIMAX_API_KEY"),
    BaseURL: "https://api.minimax.io",             // 默认
    Model:   "abab6.5s-chat",
}, logger)

// Meta Llama（多平台托管）
llamaProvider := llama.NewLlamaProvider(providers.LlamaConfig{
    APIKey:   os.Getenv("TOGETHER_API_KEY"),
    Provider: "together",                          // together/replicate/openrouter
    Model:    "meta-llama/Llama-3.3-70B-Instruct-Turbo",
}, logger)
```

## 自定义 BaseURL

支持代理、私有部署、兼容 API：

```go
// OpenAI 兼容 API（Azure、本地部署、代理）
provider := openai.NewOpenAIProvider(providers.OpenAIConfig{
    APIKey:  os.Getenv("API_KEY"),
    BaseURL: "https://your-proxy.com/v1",
    Model:   "gpt-4o",
}, logger)

// Llama 多平台托管
llamaProvider := llama.NewLlamaProvider(providers.LlamaConfig{
    APIKey:   os.Getenv("TOGETHER_API_KEY"),
    Provider: "together",  // together/replicate/openrouter
    Model:    "meta-llama/Llama-3.3-70B-Instruct-Turbo",
}, logger)

// 自动选择 BaseURL
// together  -> https://api.together.xyz
// replicate -> https://api.replicate.com
// openrouter -> https://openrouter.ai/api
```

## Provider 接口

所有 Provider 实现统一接口：

```go
type Provider interface {
    // Name 返回 Provider 名称
    Name() string
    
    // Completion 同步完成请求
    Completion(ctx context.Context, req *ChatRequest) (*ChatResponse, error)
    
    // Stream 流式完成请求
    Stream(ctx context.Context, req *ChatRequest) (<-chan StreamChunk, error)
    
    // HealthCheck 健康检查
    HealthCheck(ctx context.Context) (*HealthStatus, error)
    
    // SupportsNativeFunctionCalling 是否支持原生工具调用
    SupportsNativeFunctionCalling() bool
}
```

## 凭证覆盖

运行时动态切换 API Key：

```go
// 从上下文覆盖凭证
ctx := llm.ContextWithCredentialOverride(ctx, &llm.CredentialOverride{
    APIKey: "sk-dynamic-key",
})

// Provider 自动使用覆盖的凭证
response, err := provider.Completion(ctx, req)
```

## 弹性 Provider

`ResilientProvider` 包装原始 Provider，提供：
- 自动重试（指数退避）
- 熔断器（防止雪崩）
- 幂等性保证（防止重复请求）

```go
import "github.com/BaSui01/agentflow/llm"

// 创建幂等性管理器
idempotencyMgr := llm.NewIdempotencyManager(llm.IdempotencyConfig{
    TTL:      5 * time.Minute,
    MaxSize:  10000,
})

// 包装为弹性 Provider
resilientProvider := llm.NewResilientProviderSimple(
    provider,
    idempotencyMgr,
    logger,
)

// 使用弹性 Provider
response, err := resilientProvider.Completion(ctx, request)
```

### 自定义重试策略

```go
resilientProvider := llm.NewResilientProvider(
    provider,
    idempotencyMgr,
    llm.ResilientConfig{
        MaxRetries:      5,
        InitialDelay:    100 * time.Millisecond,
        MaxDelay:        30 * time.Second,
        BackoffFactor:   2.0,
        RetryableErrors: []string{"rate_limit", "timeout", "server_error"},
    },
    logger,
)
```

## 多 Provider 路由

`MultiProviderRouter` 支持多种路由策略：

```go
router := llm.NewMultiProviderRouter([]llm.Provider{
    openaiProvider,
    claudeProvider,
    geminiProvider,
}, llm.RouterConfig{
    Strategy:       llm.StrategyFallback,    // 故障转移
    HealthCheck:    true,                     // 健康检查
    CheckInterval:  30 * time.Second,
    Timeout:        60 * time.Second,
}, logger)
```

### 路由策略

| 策略 | 说明 |
|------|------|
| `StrategyFallback` | 故障转移：主 Provider 失败时切换到备用 |
| `StrategyRoundRobin` | 轮询：均匀分配请求 |
| `StrategyWeighted` | 加权：按权重分配请求 |
| `StrategyLowestLatency` | 最低延迟：选择响应最快的 Provider |
| `StrategyCostOptimized` | 成本优化：选择最便宜的 Provider |

### 加权路由示例

```go
router := llm.NewMultiProviderRouter(providers, llm.RouterConfig{
    Strategy: llm.StrategyWeighted,
    Weights: map[string]float64{
        "openai":    0.5,
        "anthropic": 0.3,
        "gemini":    0.2,
    },
}, logger)
```

## API Key 池化

支持多 API Key 轮换，提高并发和可用性：

```go
pool := llm.NewAPIKeyPool([]string{
    os.Getenv("OPENAI_API_KEY_1"),
    os.Getenv("OPENAI_API_KEY_2"),
    os.Getenv("OPENAI_API_KEY_3"),
}, llm.APIKeyPoolConfig{
    Strategy:        llm.KeyStrategyRoundRobin,
    HealthCheck:     true,
    CheckInterval:   60 * time.Second,
    MaxFailures:     3,
    CooldownPeriod:  5 * time.Minute,
})

// 获取可用 Key
key, err := pool.GetKey(ctx)
```

## 流式响应

所有 Provider 支持流式响应：

```go
stream, err := provider.CompletionStream(ctx, &llm.ChatRequest{
    Model: "gpt-4o",
    Messages: []llm.Message{
        {Role: llm.RoleUser, Content: "写一首诗"},
    },
})
if err != nil {
    log.Fatal(err)
}

for chunk := range stream {
    if chunk.Error != nil {
        log.Printf("Error: %v", chunk.Error)
        break
    }
    fmt.Print(chunk.Delta.Content)
}
```

## 环境变量配置

推荐使用环境变量存储敏感信息：

```bash
# OpenAI
export OPENAI_API_KEY="sk-..."

# Anthropic
export ANTHROPIC_API_KEY="sk-ant-..."

# Google
export GEMINI_API_KEY="..."

# 国产大模型
export DEEPSEEK_API_KEY="sk-..."
export QWEN_API_KEY="sk-..."
export GLM_API_KEY="..."
export HUNYUAN_API_KEY="..."
export DOUBAO_API_KEY="..."
export KIMI_API_KEY="sk-..."
```

## 成本追踪

内置成本追踪功能：

```go
import "github.com/BaSui01/agentflow/llm/observability"

tracker := observability.NewCostTracker(observability.CostConfig{
    Pricing: map[string]observability.ModelPricing{
        "gpt-4o": {
            InputPer1K:  0.005,
            OutputPer1K: 0.015,
        },
        "claude-4-opus": {
            InputPer1K:  0.015,
            OutputPer1K: 0.075,
        },
    },
})

// 记录使用
tracker.RecordUsage("gpt-4o", usage.PromptTokens, usage.CompletionTokens)

// 获取统计
stats := tracker.GetStats()
fmt.Printf("总成本: $%.4f\n", stats.TotalCost)
```

## Token 预算管理

控制 Token 使用量和成本：

```go
import "github.com/BaSui01/agentflow/llm/budget"

// 创建预算管理器
budgetMgr := budget.NewTokenBudgetManager(budget.BudgetConfig{
    MaxTokensPerRequest: 100000,    // 单次请求最大 Token
    MaxTokensPerMinute:  500000,    // 每分钟最大 Token
    MaxTokensPerHour:    5000000,   // 每小时最大 Token
    MaxTokensPerDay:     50000000,  // 每天最大 Token
    MaxCostPerRequest:   10.0,      // 单次请求最大成本
    MaxCostPerDay:       1000.0,    // 每天最大成本
    AlertThreshold:      0.8,       // 80% 时告警
    AutoThrottle:        true,      // 自动限流
    ThrottleDelay:       time.Second,
}, logger)

// 注册告警处理器
budgetMgr.OnAlert(func(alert budget.Alert) {
    log.Printf("预算告警: %s, 当前使用率: %.2f%%", alert.Message, alert.Current*100)
    // 发送通知...
})

// 请求前检查预算
err := budgetMgr.CheckBudget(ctx, estimatedTokens, estimatedCost)
if err != nil {
    log.Printf("预算不足: %v", err)
    return
}

// 请求后记录使用
budgetMgr.RecordUsage(budget.UsageRecord{
    Timestamp: time.Now(),
    Tokens:    response.Usage.TotalTokens,
    Cost:      calculateCost(response.Usage),
    Model:     "gpt-4o",
    RequestID: requestID,
})

// 获取当前状态
status := budgetMgr.GetStatus()
fmt.Printf("今日 Token 使用: %d (%.1f%%)\n", 
    status.TokensUsedDay, status.DayUtilization*100)
fmt.Printf("今日成本: $%.2f (%.1f%%)\n", 
    status.CostUsedDay, status.CostUtilization*100)
```

## 上下文管理

智能管理对话上下文，避免超出 Token 限制：

```go
import "github.com/BaSui01/agentflow/llm/context"

// 创建上下文管理器
tokenizer := context.NewSimpleTokenizer() // 或使用 tiktoken
ctxMgr := context.NewDefaultContextManager(tokenizer, logger)

// 裁剪消息以适应 Token 限制
messages := []context.Message{
    {Role: context.RoleSystem, Content: "你是一个助手"},
    {Role: context.RoleUser, Content: "问题1..."},
    {Role: context.RoleAssistant, Content: "回答1..."},
    // ... 更多消息
}

// 使用不同策略裁剪
trimmed, err := ctxMgr.PruneByStrategy(messages, 4096, context.PruneOldest)
```

### 裁剪策略

| 策略 | 说明 |
|------|------|
| `PruneOldest` | 删除最旧消息，保留 System 和最近消息 |
| `PruneByRole` | 按角色优先级裁剪（System > User/Assistant > Tool） |
| `PruneLeastImportant` | 按消息重要性裁剪（需要 Metadata.importance） |
| `PruneSlidingWindow` | 滑动窗口，保留最近 N 条消息 |
| `PruneToolCalls` | 优先删除工具调用和结果 |

### 带压缩的上下文管理

```go
// 创建摘要压缩器（需要 LLM Provider）
compressor := context.NewSummaryCompressor(provider, context.CompressionConfig{
    Threshold:     0.8,  // 80% 容量时触发压缩
    TargetRatio:   0.5,  // 压缩到 50%
    MinMessages:   5,    // 至少保留 5 条消息
})

// 创建带压缩的上下文管理器
enhancedMgr := context.NewDefaultContextManagerWithCompression(
    tokenizer,
    compressor,
    logger,
)

// 自动压缩 + 裁剪
trimmed, err := enhancedMgr.TrimMessages(messages, 4096)
```

## 背压流式处理

高吞吐场景下的流式响应处理：

```go
import "github.com/BaSui01/agentflow/llm/streaming"

// 创建背压流
stream := streaming.NewBackpressureStream(streaming.BackpressureConfig{
    BufferSize:      1024,           // 缓冲区大小
    HighWaterMark:   0.8,            // 80% 时暂停生产
    LowWaterMark:    0.2,            // 20% 时恢复生产
    SlowConsumerTTL: 30 * time.Second,
    DropPolicy:      streaming.DropPolicyBlock, // 阻塞策略
})

// 生产者（写入 Token）
go func() {
    for token := range llmStream {
        err := stream.Write(ctx, streaming.Token{
            Content:   token.Delta.Content,
            Index:     token.Index,
            Timestamp: time.Now(),
        })
        if err != nil {
            log.Printf("写入失败: %v", err)
            break
        }
    }
    stream.Close()
}()

// 消费者（读取 Token）
for {
    token, err := stream.Read(ctx)
    if err == streaming.ErrStreamClosed {
        break
    }
    if err != nil {
        log.Printf("读取失败: %v", err)
        break
    }
    fmt.Print(token.Content)
}

// 查看统计
stats := stream.Stats()
fmt.Printf("生产: %d, 消费: %d, 丢弃: %d\n", 
    stats.Produced, stats.Consumed, stats.Dropped)
```

### 丢弃策略

| 策略 | 说明 |
|------|------|
| `DropPolicyBlock` | 阻塞生产者直到缓冲区有空间 |
| `DropPolicyOldest` | 丢弃最旧的 Token |
| `DropPolicyNewest` | 丢弃最新的 Token |
| `DropPolicyError` | 返回错误 |

### 流多路复用

```go
// 创建多路复用器
multiplexer := streaming.NewStreamMultiplexer(sourceStream)

// 添加多个消费者
consumer1 := multiplexer.AddConsumer(streaming.DefaultBackpressureConfig())
consumer2 := multiplexer.AddConsumer(streaming.DefaultBackpressureConfig())

// 启动多路复用
multiplexer.Start(ctx)

// 每个消费者独立消费
go processStream(consumer1)
go processStream(consumer2)
```

## 高级路由策略

### 成本优化路由

```go
router := llm.NewMultiProviderRouter(db, providerFactory, llm.RouterOptions{
    Strategy:      llm.StrategyCostBased,
    HealthCheck:   true,
    CheckInterval: 30 * time.Second,
})

// 初始化 API Key 池
router.InitAPIKeyPools(ctx)

// 选择最便宜的健康 Provider
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyCostBased)
```

### 健康优先路由

```go
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyHealthBased)
```

### QPS 负载均衡

```go
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyQPSBased)
```

## 最佳实践

1. **生产环境必须使用 ResilientProvider**：提供重试、熔断保护
2. **多 Provider 路由**：避免单点故障，提高可用性
3. **API Key 池化**：提高并发能力，分散限流风险
4. **环境变量**：不要在代码中硬编码 API Key
5. **成本监控**：追踪 Token 使用和成本
6. **健康检查**：定期检查 Provider 可用性
7. **Token 预算**：设置预算限制，防止成本失控
8. **上下文管理**：合理裁剪消息，避免超出限制
9. **背压处理**：高吞吐场景使用背压流
