# Provider 配置指南

AgentFlow 提供统一的 LLM Provider 抽象层，支持 13+ 主流大模型提供商，具备弹性容错、多 Provider 路由、API Key 池化等企业级特性。

## 支持的 Provider

| Provider | 默认模型 | 默认 BaseURL | 特点 |
|----------|----------|--------------|------|
| OpenAI | gpt-5.2 | https://api.openai.com | 工具调用、多模态、Responses API |
| Anthropic | claude-opus-4.5 | https://api.anthropic.com | 长上下文、思维链、Thought Signatures |
| Gemini | gemini-3-pro | https://generativelanguage.googleapis.com | 多模态、1M tokens 上下文 |
| DeepSeek | deepseek-chat | https://api.deepseek.com | 高性价比、deepseek-reasoner 推理模式 |
| Qwen | qwen3-235b-a22b | https://dashscope.aliyuncs.com | 中文优化、DashScope API |
| GLM | glm-4-plus | https://open.bigmodel.cn | 智谱 AI、中文优化 |
| Grok | grok-beta | https://api.x.ai | xAI、实时信息 |
| MiniMax | abab6.5s-chat | https://api.minimax.io | XML 工具调用格式 |
| Mistral | mistral-large | https://api.mistral.ai | 欧洲合规、OpenAI 兼容 |
| Hunyuan | hunyuan-pro | https://api.hunyuan.cloud.tencent.com/v1 | 腾讯混元、OpenAI 兼容 |
| Kimi | moonshot-v1 | https://api.moonshot.cn | 长上下文、OpenAI 兼容 |
| Llama | llama-3.3-70b | https://api.together.xyz | 多平台托管（Together/Replicate/OpenRouter） |
| Doubao | Doubao-1.5-pro-32k | https://ark.cn-beijing.volces.com | 字节跳动火山方舟 |

## API 格式分类

### OpenAI 兼容 API
以下 Provider 使用 OpenAI 兼容 API，可复用相同的请求/响应格式：
- OpenAI、DeepSeek、Qwen、GLM、Grok、Mistral、Hunyuan、Kimi、Llama、Doubao

### 自定义 API
- **Anthropic Claude**: 使用 `x-api-key` 认证，system 消息单独传递，SSE 流式格式不同
- **Google Gemini**: 使用 `x-goog-api-key` 认证，消息格式为 `contents` 数组
- **MiniMax**: 使用 XML 格式的工具调用 `<tool_calls>...</tool_calls>`

## 基础配置

### OpenAI

```go
import (
    "github.com/BaSui01/agentflow/llm/providers"
    "github.com/BaSui01/agentflow/llm/providers/openai"
)

provider := openai.NewOpenAIProvider(providers.OpenAIConfig{
    APIKey:          os.Getenv("OPENAI_API_KEY"),
    Model:           "gpt-5.2",              // 2026 默认模型
    Organization:    "org-xxx",              // 可选：组织 ID
    Timeout:         60 * time.Second,
    UseResponsesAPI: true,                   // 启用 Responses API（有状态对话）
}, logger)

// Responses API 支持有状态对话
ctx := context.WithValue(ctx, "previous_response_id", "resp_xxx")
response, _ := provider.Completion(ctx, req)
```

### Anthropic Claude

```go
import (
    "github.com/BaSui01/agentflow/llm/providers"
    "github.com/BaSui01/agentflow/llm/providers/anthropic"
)

provider := anthropic.NewClaudeProvider(providers.ClaudeConfig{
    APIKey:  os.Getenv("ANTHROPIC_API_KEY"),
    Model:   "claude-opus-4.5-20260105",     // 2026 默认模型
    Timeout: 120 * time.Second,              // Claude 响应较慢，建议 120s
}, logger)

// Claude 特有功能：混合推理模式、Thought Signatures
req := &llm.ChatRequest{
    Messages:          messages,
    ReasoningMode:     "extended",           // 2026: fast/extended
    ThoughtSignatures: []string{"sig1"},     // 2026: Thought Signatures
}
```

### Google Gemini

```go
import (
    "github.com/BaSui01/agentflow/llm/providers"
    "github.com/BaSui01/agentflow/llm/providers/gemini"
)

provider := gemini.NewGeminiProvider(providers.GeminiConfig{
    APIKey:  os.Getenv("GEMINI_API_KEY"),
    Model:   "gemini-3-pro",                 // 2026 默认模型
    Timeout: 60 * time.Second,
}, logger)

// Gemini 支持多模态（图片、音频、视频）
// 消息格式自动转换为 Gemini 的 contents 数组
```

### DeepSeek

```go
import (
    "github.com/BaSui01/agentflow/llm/providers"
    "github.com/BaSui01/agentflow/llm/providers/deepseek"
)

provider := deepseek.NewDeepSeekProvider(providers.DeepSeekConfig{
    APIKey:  os.Getenv("DEEPSEEK_API_KEY"),
    Model:   "deepseek-chat",
    Timeout: 60 * time.Second,
}, logger)

// DeepSeek 推理模式：自动切换到 deepseek-reasoner
req := &llm.ChatRequest{
    Messages:      messages,
    ReasoningMode: "thinking",               // 自动使用 deepseek-reasoner
}
```

### 国产大模型

```go
// 阿里通义千问（DashScope API）
qwenProvider := qwen.NewQwenProvider(providers.QwenConfig{
    APIKey:  os.Getenv("QWEN_API_KEY"),
    BaseURL: "https://dashscope.aliyuncs.com",  // 默认
    Model:   "qwen3-235b-a22b",                 // 2026 默认
}, logger)

// 智谱 GLM
glmProvider := glm.NewGLMProvider(providers.GLMConfig{
    APIKey:  os.Getenv("GLM_API_KEY"),
    BaseURL: "https://open.bigmodel.cn",        // 默认
    Model:   "glm-4-plus",
}, logger)

// 腾讯混元（OpenAI 兼容）
hunyuanProvider := hunyuan.NewHunyuanProvider(providers.HunyuanConfig{
    APIKey:  os.Getenv("HUNYUAN_API_KEY"),
    BaseURL: "https://api.hunyuan.cloud.tencent.com/v1",
    Model:   "hunyuan-pro",
}, logger)

// 字节豆包（火山方舟）
doubaoProvider := doubao.NewDoubaoProvider(providers.DoubaoConfig{
    APIKey:  os.Getenv("DOUBAO_API_KEY"),
    BaseURL: "https://ark.cn-beijing.volces.com",  // 默认
    Model:   "Doubao-1.5-pro-32k",
}, logger)

// 月之暗面 Kimi（OpenAI 兼容）
kimiProvider := kimi.NewKimiProvider(providers.KimiConfig{
    APIKey:  os.Getenv("KIMI_API_KEY"),
    BaseURL: "https://api.moonshot.cn",            // 默认
    Model:   "moonshot-v1-128k",
}, logger)
```

### 其他 Provider

```go
// xAI Grok
grokProvider := grok.NewGrokProvider(providers.GrokConfig{
    APIKey:  os.Getenv("GROK_API_KEY"),
    BaseURL: "https://api.x.ai",                   // 默认
    Model:   "grok-beta",
}, logger)

// Mistral AI（欧洲合规）
mistralProvider := mistral.NewMistralProvider(providers.MistralConfig{
    APIKey:  os.Getenv("MISTRAL_API_KEY"),
    BaseURL: "https://api.mistral.ai",             // 默认
    Model:   "mistral-large",
}, logger)

// MiniMax（XML 工具调用格式）
minimaxProvider := minimax.NewMiniMaxProvider(providers.MiniMaxConfig{
    APIKey:  os.Getenv("MINIMAX_API_KEY"),
    BaseURL: "https://api.minimax.io",             // 默认
    Model:   "abab6.5s-chat",
}, logger)

// Meta Llama（多平台托管）
llamaProvider := llama.NewLlamaProvider(providers.LlamaConfig{
    APIKey:   os.Getenv("TOGETHER_API_KEY"),
    Provider: "together",                          // together/replicate/openrouter
    Model:    "meta-llama/Llama-3.3-70B-Instruct-Turbo",
}, logger)
```

## 自定义 BaseURL

支持代理、私有部署、兼容 API：

```go
// OpenAI 兼容 API（Azure、本地部署、代理）
provider := openai.NewOpenAIProvider(providers.OpenAIConfig{
    APIKey:  os.Getenv("API_KEY"),
    BaseURL: "https://your-proxy.com/v1",
    Model:   "gpt-4o",
}, logger)

// Llama 多平台托管
llamaProvider := llama.NewLlamaProvider(providers.LlamaConfig{
    APIKey:   os.Getenv("TOGETHER_API_KEY"),
    Provider: "together",  // together/replicate/openrouter
    Model:    "meta-llama/Llama-3.3-70B-Instruct-Turbo",
}, logger)

// 自动选择 BaseURL
// together  -> https://api.together.xyz
// replicate -> https://api.replicate.com
// openrouter -> https://openrouter.ai/api
```

## Provider 接口

所有 Provider 实现统一接口：

```go
type Provider interface {
    // Name 返回 Provider 名称
    Name() string
    
    // Completion 同步完成请求
    Completion(ctx context.Context, req *ChatRequest) (*ChatResponse, error)
    
    // Stream 流式完成请求
    Stream(ctx context.Context, req *ChatRequest) (<-chan StreamChunk, error)
    
    // HealthCheck 健康检查
    HealthCheck(ctx context.Context) (*HealthStatus, error)
    
    // SupportsNativeFunctionCalling 是否支持原生工具调用
    SupportsNativeFunctionCalling() bool
}
```

## 凭证覆盖

运行时动态切换 API Key：

```go
// 从上下文覆盖凭证
ctx := llm.ContextWithCredentialOverride(ctx, &llm.CredentialOverride{
    APIKey: "sk-dynamic-key",
})

// Provider 自动使用覆盖的凭证
response, err := provider.Completion(ctx, req)
```

## 弹性 Provider

`ResilientProvider` 包装原始 Provider，提供：
- 自动重试（指数退避）
- 熔断器（防止雪崩）
- 幂等性保证（防止重复请求）

```go
import "github.com/BaSui01/agentflow/llm"

// 创建幂等性管理器
idempotencyMgr := llm.NewIdempotencyManager(llm.IdempotencyConfig{
    TTL:      5 * time.Minute,
    MaxSize:  10000,
})

// 包装为弹性 Provider
resilientProvider := llm.NewResilientProviderSimple(
    provider,
    idempotencyMgr,
    logger,
)

// 使用弹性 Provider
response, err := resilientProvider.Completion(ctx, request)
```

### 自定义重试策略

```go
resilientProvider := llm.NewResilientProvider(
    provider,
    idempotencyMgr,
    llm.ResilientConfig{
        MaxRetries:      5,
        InitialDelay:    100 * time.Millisecond,
        MaxDelay:        30 * time.Second,
        BackoffFactor:   2.0,
        RetryableErrors: []string{"rate_limit", "timeout", "server_error"},
    },
    logger,
)
```

## 多 Provider 路由（DB 驱动）

`llm.MultiProviderRouter` 用数据库（`sc_llm_*`）维护 Provider/Model 映射，并为每个 Provider 管理一个 API Key 池，从而实现“同一个模型名”在多个 Provider 间的智能路由。

### 路由策略

| 策略 | 说明 |
|------|------|
| `StrategyCostBased` | 成本优先：优先选择（输入+输出）价格最低的 Provider |
| `StrategyHealthBased` | 健康优先：优先选择健康分最高的 Provider（分数相同按 `Priority`） |
| `StrategyQPSBased` | QPS 负载均衡：优先选择当前 QPS 最低的 Provider（相同按 `Priority`） |

### 最小可运行示例

```go
package main

import (
    "context"
    "fmt"
    "os"

    "github.com/BaSui01/agentflow/llm"
    "github.com/BaSui01/agentflow/llm/providers"
    openaiprov "github.com/BaSui01/agentflow/llm/providers/openai"
    "github.com/glebarez/sqlite"
    "go.uber.org/zap"
    "gorm.io/gorm"
)

func main() {
    logger, _ := zap.NewDevelopment()
    defer logger.Sync()

    ctx := context.Background()

    db, err := gorm.Open(sqlite.Open("file::memory:?cache=shared"), &gorm.Config{})
    if err != nil {
        panic(err)
    }
    if err := llm.InitDatabase(db); err != nil {
        panic(err)
    }

    provider := llm.LLMProvider{Code: "openai", Name: "OpenAI", Status: llm.LLMProviderStatusActive}
    if err := db.Create(&provider).Error; err != nil {
        panic(err)
    }
    model := llm.LLMModel{ModelName: "gpt-4o", DisplayName: "GPT-4o", Enabled: true}
    if err := db.Create(&model).Error; err != nil {
        panic(err)
    }
    if err := db.Create(&llm.LLMProviderModel{
        ModelID:         model.ID,
        ProviderID:      provider.ID,
        RemoteModelName: "gpt-4o",
        BaseURL:         "https://api.openai.com",
        PriceInput:      0.001,
        PriceCompletion: 0.002,
        Priority:        10,
        Enabled:         true,
    }).Error; err != nil {
        panic(err)
    }

    apiKey := os.Getenv("OPENAI_API_KEY")
    if apiKey == "" {
        apiKey = "sk-xxx" // 演示用 key（未设置真实 key 时不进行线上调用）
    }
    if err := db.Create(&llm.LLMProviderAPIKey{
        ProviderID: provider.ID,
        APIKey:     apiKey,
        Label:      "default",
        Priority:   10,
        Weight:     100,
        Enabled:    true,
    }).Error; err != nil {
        panic(err)
    }

    factory := llm.NewDefaultProviderFactory()
    factory.RegisterProvider("openai", func(apiKey, baseURL string) (llm.Provider, error) {
        return openaiprov.NewOpenAIProvider(providers.OpenAIConfig{
            APIKey:  apiKey,
            BaseURL: baseURL,
        }, logger), nil
    })

    router := llm.NewMultiProviderRouter(db, factory, llm.RouterOptions{Logger: logger})
    if err := router.InitAPIKeyPools(ctx); err != nil {
        panic(err)
    }

    sel, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyCostBased)
    if err != nil {
        panic(err)
    }

    fmt.Printf("selected provider=%s model=%s\n", sel.ProviderCode, sel.ModelName)
}
```

## API Key 池化

`llm.APIKeyPool` 是按 Provider 维度的、数据库驱动的 API Key 池。通常你只需要调用 `MultiProviderRouter.InitAPIKeyPools(...)` 初始化并加载所有 Provider 的 Key 池；如果需要也可直接使用：

```go
pool := llm.NewAPIKeyPool(db, providerID, llm.StrategyWeightedRandom, logger)
_ = pool.LoadKeys(ctx)

key, err := pool.SelectKey(ctx)
if err != nil {
    // handle err
}

// 记录成功/失败（异步更新 DB）
_ = pool.RecordSuccess(ctx, key.ID)
```

## 流式响应

所有 Provider 支持通过 `Provider.Stream` 获取流式响应：

```go
stream, err := provider.Stream(ctx, &llm.ChatRequest{
    Model: "gpt-4o",
    Messages: []llm.Message{
        {Role: llm.RoleUser, Content: "写一首诗"},
    },
})
if err != nil {
    log.Fatal(err)
}

for chunk := range stream {
    if chunk.Err != nil {
        log.Printf("Error: %v", chunk.Err)
        break
    }
    fmt.Print(chunk.Delta.Content)
}
```

## 环境变量配置

推荐使用环境变量存储敏感信息：

```bash
# OpenAI
export OPENAI_API_KEY="sk-..."

# Anthropic
export ANTHROPIC_API_KEY="sk-ant-..."

# Google
export GEMINI_API_KEY="..."

# 国产大模型
export DEEPSEEK_API_KEY="sk-..."
export QWEN_API_KEY="sk-..."
export GLM_API_KEY="..."
export HUNYUAN_API_KEY="..."
export DOUBAO_API_KEY="..."
export KIMI_API_KEY="sk-..."
```

## 成本追踪

内置成本追踪功能：

```go
import "github.com/BaSui01/agentflow/llm/observability"

tracker := observability.NewCostTracker(observability.CostConfig{
    Pricing: map[string]observability.ModelPricing{
        "gpt-4o": {
            InputPer1K:  0.005,
            OutputPer1K: 0.015,
        },
        "claude-4-opus": {
            InputPer1K:  0.015,
            OutputPer1K: 0.075,
        },
    },
})

// 记录使用
tracker.RecordUsage("gpt-4o", usage.PromptTokens, usage.CompletionTokens)

// 获取统计
stats := tracker.GetStats()
fmt.Printf("总成本: $%.4f\n", stats.TotalCost)
```

## Token 预算管理

控制 Token 使用量和成本：

```go
import "github.com/BaSui01/agentflow/llm/budget"

// 创建预算管理器
budgetMgr := budget.NewTokenBudgetManager(budget.BudgetConfig{
    MaxTokensPerRequest: 100000,    // 单次请求最大 Token
    MaxTokensPerMinute:  500000,    // 每分钟最大 Token
    MaxTokensPerHour:    5000000,   // 每小时最大 Token
    MaxTokensPerDay:     50000000,  // 每天最大 Token
    MaxCostPerRequest:   10.0,      // 单次请求最大成本
    MaxCostPerDay:       1000.0,    // 每天最大成本
    AlertThreshold:      0.8,       // 80% 时告警
    AutoThrottle:        true,      // 自动限流
    ThrottleDelay:       time.Second,
}, logger)

// 注册告警处理器
budgetMgr.OnAlert(func(alert budget.Alert) {
    log.Printf("预算告警: %s, 当前使用率: %.2f%%", alert.Message, alert.Current*100)
    // 发送通知...
})

// 请求前检查预算
err := budgetMgr.CheckBudget(ctx, estimatedTokens, estimatedCost)
if err != nil {
    log.Printf("预算不足: %v", err)
    return
}

// 请求后记录使用
budgetMgr.RecordUsage(budget.UsageRecord{
    Timestamp: time.Now(),
    Tokens:    response.Usage.TotalTokens,
    Cost:      calculateCost(response.Usage),
    Model:     "gpt-4o",
    RequestID: requestID,
})

// 获取当前状态
status := budgetMgr.GetStatus()
fmt.Printf("今日 Token 使用: %d (%.1f%%)\n", 
    status.TokensUsedDay, status.DayUtilization*100)
fmt.Printf("今日成本: $%.2f (%.1f%%)\n", 
    status.CostUsedDay, status.CostUtilization*100)
```

## 上下文管理

AgentFlow 提供上下文工程能力（`agent/context`），用于在接近上下文窗口上限时自动压缩/截断消息历史，避免超出 Token 限制。

```go
import (
    "context"
    "fmt"

    agentcontext "github.com/BaSui01/agentflow/agent/context"
    "github.com/BaSui01/agentflow/types"
)

engineer := agentcontext.New(agentcontext.DefaultConfig(), logger)

messages := []types.Message{
    {Role: types.RoleSystem, Content: "你是一个助手"},
    {Role: types.RoleUser, Content: "问题1..."},
    {Role: types.RoleAssistant, Content: "回答1..."},
    // ... 更多消息
}

status := engineer.GetStatus(messages)
fmt.Printf("tokens=%d usage=%.2f%% recommendation=%s\n",
    status.CurrentTokens, status.UsageRatio*100, status.Recommendation)

trimmed, err := engineer.MustFit(context.Background(), messages, "当前问题")
```

> 注：更细粒度的“按策略裁剪（滑窗/按角色/按重要性）”API 与 LLM 摘要压缩接口已预留，但当前版本未以 `PruneByStrategy` 形式对外暴露。

## 背压流式处理

高吞吐场景下的流式响应处理：

```go
import "github.com/BaSui01/agentflow/llm/streaming"

// 创建背压流
stream := streaming.NewBackpressureStream(streaming.BackpressureConfig{
    BufferSize:      1024,           // 缓冲区大小
    HighWaterMark:   0.8,            // 80% 时暂停生产
    LowWaterMark:    0.2,            // 20% 时恢复生产
    SlowConsumerTTL: 30 * time.Second,
    DropPolicy:      streaming.DropPolicyBlock, // 阻塞策略
})

// 生产者（写入 Token）
go func() {
    for token := range llmStream {
        err := stream.Write(ctx, streaming.Token{
            Content:   token.Delta.Content,
            Index:     token.Index,
            Timestamp: time.Now(),
        })
        if err != nil {
            log.Printf("写入失败: %v", err)
            break
        }
    }
    stream.Close()
}()

// 消费者（读取 Token）
for {
    token, err := stream.Read(ctx)
    if err == streaming.ErrStreamClosed {
        break
    }
    if err != nil {
        log.Printf("读取失败: %v", err)
        break
    }
    fmt.Print(token.Content)
}

// 查看统计
stats := stream.Stats()
fmt.Printf("生产: %d, 消费: %d, 丢弃: %d\n", 
    stats.Produced, stats.Consumed, stats.Dropped)
```

### 丢弃策略

| 策略 | 说明 |
|------|------|
| `DropPolicyBlock` | 阻塞生产者直到缓冲区有空间 |
| `DropPolicyOldest` | 丢弃最旧的 Token |
| `DropPolicyNewest` | 丢弃最新的 Token |
| `DropPolicyError` | 返回错误 |

### 流多路复用

```go
// 创建多路复用器
multiplexer := streaming.NewStreamMultiplexer(sourceStream)

// 添加多个消费者
consumer1 := multiplexer.AddConsumer(streaming.DefaultBackpressureConfig())
consumer2 := multiplexer.AddConsumer(streaming.DefaultBackpressureConfig())

// 启动多路复用
multiplexer.Start(ctx)

// 每个消费者独立消费
go processStream(consumer1)
go processStream(consumer2)
```

## 高级路由策略

### 成本优化路由

```go
router := llm.NewMultiProviderRouter(db, providerFactory, llm.RouterOptions{
    Logger:              logger,
    HealthCheckInterval: 30 * time.Second,
    HealthCheckTimeout:  10 * time.Second,
})

// 初始化 API Key 池
if err := router.InitAPIKeyPools(ctx); err != nil {
    // handle err
}

// 选择最便宜的健康 Provider
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyCostBased)
```

### 健康优先路由

```go
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyHealthBased)
```

### QPS 负载均衡

```go
selection, err := router.SelectProviderWithModel(ctx, "gpt-4o", llm.StrategyQPSBased)
```

## 最佳实践

1. **生产环境必须使用 ResilientProvider**：提供重试、熔断保护
2. **多 Provider 路由**：避免单点故障，提高可用性
3. **API Key 池化**：提高并发能力，分散限流风险
4. **环境变量**：不要在代码中硬编码 API Key
5. **成本监控**：追踪 Token 使用和成本
6. **健康检查**：定期检查 Provider 可用性
7. **Token 预算**：设置预算限制，防止成本失控
8. **上下文管理**：合理裁剪消息，避免超出限制
9. **背压处理**：高吞吐场景使用背压流

---

## A/B 测试路由

`llm/router/ab_router.go` 提供了 A/B 测试路由器，支持多变体流量分配、粘性路由和动态权重调整。

### ABRouter 配置

```go
import (
    "github.com/BaSui01/agentflow/llm/router"
    "go.uber.org/zap"
)

logger, _ := zap.NewDevelopment()

abRouter, err := router.NewABRouter(router.ABTestConfig{
    Name: "gpt4o-vs-claude",
    Variants: []router.ABVariant{
        {
            Name:     "control",
            Provider: openaiProvider,  // 已创建的 OpenAI Provider
            Weight:   70,              // 70% 流量
        },
        {
            Name:     "experiment",
            Provider: claudeProvider,  // 已创建的 Claude Provider
            Weight:   30,              // 30% 流量
        },
    },
    StickyRouting: true,       // 同一用户始终路由到同一变体
    StickyKey:     "user_id",  // 粘性路由键：user_id / session_id / tenant_id
}, logger)
```

### 动态权重调整

```go
// 运行时调整流量分配（权重必须加和为 100）
err := abRouter.UpdateWeights(map[string]int{
    "control":    50,
    "experiment": 50,
})
```

### 指标收集

```go
// 获取每个变体的指标
metrics := abRouter.GetMetrics()
for name, m := range metrics {
    fmt.Printf("变体 %s: 请求数=%d, 成功率=%.2f, 平均延迟=%.1fms\n",
        name, m.TotalRequests, m.GetSuccessRate(), m.GetAvgLatencyMs())
}

// 获取汇总报告
report := abRouter.GetReport()
```

> ABRouter 实现了 `llm.Provider` 接口，可以直接替代普通 Provider 使用。

---

## Provider 重试包装器

`llm/providers/retry_wrapper.go` 提供了 `RetryableProvider`，为任意 Provider 添加指数退避重试能力。

### 配置与使用

```go
import "github.com/BaSui01/agentflow/llm/providers"

// 使用默认配置（MaxRetries=3, InitialDelay=1s, MaxDelay=30s, BackoffFactor=2.0）
retryProvider := providers.NewRetryableProvider(
    baseProvider,
    providers.DefaultRetryConfig(),
    logger,
)

// 自定义配置
retryProvider := providers.NewRetryableProvider(
    baseProvider,
    providers.RetryConfig{
        MaxRetries:    5,
        InitialDelay:  500 * time.Millisecond,
        MaxDelay:      60 * time.Second,
        BackoffFactor: 2.0,
        RetryableOnly: true,  // 仅重试标记为 Retryable 的错误
    },
    logger,
)

// 像普通 Provider 一样使用
resp, err := retryProvider.Completion(ctx, req)
```

> `RetryableProvider` 实现了 `llm.Provider` 接口。Stream 方法仅重试连接建立阶段，流式传输中的错误不会重试。

---

## 统一 Token 计数器

`llm/tokenizer/` 提供统一的 Token 计数接口，支持精确计数（tiktoken）和快速估算（CJK 估算器）。

### Tokenizer 接口

```go
import "github.com/BaSui01/agentflow/llm/tokenizer"

// 接口定义
type Tokenizer interface {
    CountTokens(text string) (int, error)
    CountMessages(messages []Message) (int, error)
    Encode(text string) ([]int, error)
    Decode(tokens []int) (string, error)
    MaxTokens() int
    Name() string
}
```

### tiktoken 适配器（精确计数）

```go
// 为 OpenAI 模型创建 tiktoken tokenizer
tok, err := tokenizer.NewTiktokenTokenizer("gpt-4o")

count, _ := tok.CountTokens("Hello, 你好世界！")
fmt.Println(count) // 精确 token 数

// 批量注册所有已知 OpenAI 模型的 tokenizer
tokenizer.RegisterOpenAITokenizers()
```

### CJK 估算器（无需下载模型数据）

```go
// 创建估算器（适用于非 OpenAI 模型或离线环境）
est := tokenizer.NewEstimatorTokenizer("deepseek-chat", 32768)

// CJK 字符 ~1.5 字符/token，ASCII ~4 字符/token
count, _ := est.CountTokens("你好世界 Hello World")
```

### 全局注册表

```go
// 注册自定义 tokenizer
tokenizer.RegisterTokenizer("my-model", myTokenizer)

// 获取已注册的 tokenizer（支持前缀匹配）
tok, err := tokenizer.GetTokenizer("gpt-4o-mini")

// 获取 tokenizer，未注册时自动回退到估算器
tok := tokenizer.GetTokenizerOrEstimator("unknown-model")
```
