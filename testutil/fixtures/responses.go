// =============================================================================
// ğŸ“¦ æµ‹è¯•æ•°æ®å·¥å‚ - LLM å“åº”æµ‹è¯•æ•°æ®
// =============================================================================
// æä¾›é¢„å®šä¹‰çš„ LLM å“åº”æ•°æ®ï¼Œç”¨äºæµ‹è¯•
// =============================================================================
package fixtures

import (
	"time"

	"github.com/BaSui01/agentflow/llm"
	"github.com/BaSui01/agentflow/types"
)

// =============================================================================
// ğŸ¯ ChatResponse å·¥å‚
// =============================================================================

// SimpleResponse è¿”å›ç®€å•çš„æ–‡æœ¬å“åº”
func SimpleResponse(content string) *llm.ChatResponse {
	return &llm.ChatResponse{
		ID:       "resp-001",
		Provider: "mock",
		Model:    "gpt-4",
		Choices: []llm.ChatChoice{
			{
				Index:        0,
				FinishReason: "stop",
				Message: types.Message{
					Role:    types.RoleAssistant,
					Content: content,
				},
			},
		},
		Usage: llm.ChatUsage{
			PromptTokens:     10,
			CompletionTokens: 20,
			TotalTokens:      30,
		},
		CreatedAt: time.Now(),
	}
}

// ResponseWithUsage è¿”å›å¸¦è‡ªå®šä¹‰ Token ä½¿ç”¨é‡çš„å“åº”
func ResponseWithUsage(content string, promptTokens, completionTokens int) *llm.ChatResponse {
	resp := SimpleResponse(content)
	resp.Usage = llm.ChatUsage{
		PromptTokens:     promptTokens,
		CompletionTokens: completionTokens,
		TotalTokens:      promptTokens + completionTokens,
	}
	return resp
}

// ResponseWithToolCalls è¿”å›å¸¦å·¥å…·è°ƒç”¨çš„å“åº”
func ResponseWithToolCalls(content string, toolCalls []types.ToolCall) *llm.ChatResponse {
	return &llm.ChatResponse{
		ID:       "resp-tool-001",
		Provider: "mock",
		Model:    "gpt-4",
		Choices: []llm.ChatChoice{
			{
				Index:        0,
				FinishReason: "tool_calls",
				Message: types.Message{
					Role:      types.RoleAssistant,
					Content:   content,
					ToolCalls: toolCalls,
				},
			},
		},
		Usage: llm.ChatUsage{
			PromptTokens:     50,
			CompletionTokens: 100,
			TotalTokens:      150,
		},
		CreatedAt: time.Now(),
	}
}

// ResponseWithSingleToolCall è¿”å›å¸¦å•ä¸ªå·¥å…·è°ƒç”¨çš„å“åº”
func ResponseWithSingleToolCall(content, toolName, toolID string, args []byte) *llm.ChatResponse {
	return ResponseWithToolCalls(content, []types.ToolCall{
		{
			ID:        toolID,
			Name:      toolName,
			Arguments: args,
		},
	})
}

// TruncatedResponse è¿”å›å› é•¿åº¦é™åˆ¶è€Œæˆªæ–­çš„å“åº”
func TruncatedResponse(content string) *llm.ChatResponse {
	resp := SimpleResponse(content)
	resp.Choices[0].FinishReason = "length"
	resp.Usage = llm.ChatUsage{
		PromptTokens:     100,
		CompletionTokens: 4096,
		TotalTokens:      4196,
	}
	return resp
}

// ContentFilteredResponse è¿”å›è¢«å†…å®¹è¿‡æ»¤çš„å“åº”
func ContentFilteredResponse() *llm.ChatResponse {
	return &llm.ChatResponse{
		ID:       "resp-filtered-001",
		Provider: "mock",
		Model:    "gpt-4",
		Choices: []llm.ChatChoice{
			{
				Index:        0,
				FinishReason: "content_filter",
				Message: types.Message{
					Role:    types.RoleAssistant,
					Content: "",
				},
			},
		},
		Usage: llm.ChatUsage{
			PromptTokens:     50,
			CompletionTokens: 0,
			TotalTokens:      50,
		},
		CreatedAt: time.Now(),
	}
}

// =============================================================================
// ğŸŒŠ StreamChunk å·¥å‚
// =============================================================================

// TextChunk åˆ›å»ºæ–‡æœ¬æµå¼å—
func TextChunk(content string, finishReason string) llm.StreamChunk {
	return llm.StreamChunk{
		ID:       "chunk-001",
		Provider: "mock",
		Model:    "gpt-4",
		Delta: types.Message{
			Role:    types.RoleAssistant,
			Content: content,
		},
		FinishReason: finishReason,
	}
}

// ToolCallChunk åˆ›å»ºå·¥å…·è°ƒç”¨æµå¼å—
func ToolCallChunk(toolCall types.ToolCall, finishReason string) llm.StreamChunk {
	return llm.StreamChunk{
		ID:       "chunk-tool-001",
		Provider: "mock",
		Model:    "gpt-4",
		Delta: types.Message{
			Role:      types.RoleAssistant,
			ToolCalls: []types.ToolCall{toolCall},
		},
		FinishReason: finishReason,
	}
}

// ErrorChunk åˆ›å»ºé”™è¯¯æµå¼å—
func ErrorChunk(err *types.Error) llm.StreamChunk {
	return llm.StreamChunk{
		ID:           "chunk-error-001",
		Provider:     "mock",
		Model:        "gpt-4",
		FinishReason: "error",
		Err:          err,
	}
}

// SimpleStreamChunks è¿”å›ç®€å•çš„æµå¼å—åºåˆ—
func SimpleStreamChunks(content string, chunkSize int) []llm.StreamChunk {
	var chunks []llm.StreamChunk

	for i := 0; i < len(content); i += chunkSize {
		end := i + chunkSize
		if end > len(content) {
			end = len(content)
		}

		chunk := content[i:end]
		finishReason := ""
		if end >= len(content) {
			finishReason = "stop"
		}

		chunks = append(chunks, TextChunk(chunk, finishReason))
	}

	// ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå—
	if len(chunks) == 0 {
		chunks = append(chunks, TextChunk("", "stop"))
	}

	return chunks
}

// WordByWordChunks è¿”å›é€è¯çš„æµå¼å—åºåˆ—
func WordByWordChunks(words []string) []llm.StreamChunk {
	chunks := make([]llm.StreamChunk, len(words))
	for i, word := range words {
		content := word
		if i < len(words)-1 {
			content += " "
		}
		finishReason := ""
		if i == len(words)-1 {
			finishReason = "stop"
		}
		chunks[i] = TextChunk(content, finishReason)
	}
	return chunks
}

// =============================================================================
// ğŸ“Š Token ä½¿ç”¨é‡å·¥å‚
// =============================================================================

// SmallUsage è¿”å›å°é‡ Token ä½¿ç”¨
func SmallUsage() llm.ChatUsage {
	return llm.ChatUsage{
		PromptTokens:     10,
		CompletionTokens: 20,
		TotalTokens:      30,
	}
}

// MediumUsage è¿”å›ä¸­ç­‰ Token ä½¿ç”¨
func MediumUsage() llm.ChatUsage {
	return llm.ChatUsage{
		PromptTokens:     500,
		CompletionTokens: 1000,
		TotalTokens:      1500,
	}
}

// LargeUsage è¿”å›å¤§é‡ Token ä½¿ç”¨
func LargeUsage() llm.ChatUsage {
	return llm.ChatUsage{
		PromptTokens:     4000,
		CompletionTokens: 4096,
		TotalTokens:      8096,
	}
}

// CustomUsage è¿”å›è‡ªå®šä¹‰ Token ä½¿ç”¨
func CustomUsage(prompt, completion int) llm.ChatUsage {
	return llm.ChatUsage{
		PromptTokens:     prompt,
		CompletionTokens: completion,
		TotalTokens:      prompt + completion,
	}
}

// =============================================================================
// ğŸ­ é¢„è®¾å“åº”åœºæ™¯
// =============================================================================

// GreetingResponse è¿”å›é—®å€™å“åº”
func GreetingResponse() *llm.ChatResponse {
	return SimpleResponse("Hello! How can I assist you today?")
}

// CalculationResponse è¿”å›è®¡ç®—å“åº”
func CalculationResponse(result string) *llm.ChatResponse {
	return SimpleResponse("The result is: " + result)
}

// SearchResultResponse è¿”å›æœç´¢ç»“æœå“åº”
func SearchResultResponse(results []string) *llm.ChatResponse {
	content := "Here are the search results:\n"
	for i, r := range results {
		content += string(rune('1'+i)) + ". " + r + "\n"
	}
	return SimpleResponse(content)
}

// ErrorExplanationResponse è¿”å›é”™è¯¯è§£é‡Šå“åº”
func ErrorExplanationResponse(errorMsg string) *llm.ChatResponse {
	return SimpleResponse("I encountered an error: " + errorMsg + ". Let me try a different approach.")
}

// ThinkingResponse è¿”å›æ€è€ƒè¿‡ç¨‹å“åº”
func ThinkingResponse(thinking, conclusion string) *llm.ChatResponse {
	return SimpleResponse("Let me think about this...\n\n" + thinking + "\n\nConclusion: " + conclusion)
}

// RefusalResponse è¿”å›æ‹’ç»å“åº”
func RefusalResponse(reason string) *llm.ChatResponse {
	return SimpleResponse("I'm sorry, but I can't help with that request. " + reason)
}

// ClarificationResponse è¿”å›æ¾„æ¸…è¯·æ±‚å“åº”
func ClarificationResponse(question string) *llm.ChatResponse {
	return SimpleResponse("I need some clarification before I can help. " + question)
}
